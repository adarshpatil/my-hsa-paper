\section{Motivation} \label{motivation}
As seen in section \ref{introduction}, the large capacity provided by the stacked DRAM are in line with the working set requirements of IHS processors. Using this capacity as a hardware managed cache could provide performance gains without any application modifications. The bandwidth benefits and modestly improved latency provided by the DRAMCache help improve performance of IHS processors over on-chip SRAM caches of reasonable sizes \cite{amd-exascale1}. However, the organization of the DRAMCache for this type of heterogeneous processors is not immediately intuitive.
\par The large storage requirement for metadata in DRAMCaches - of the order of the last level SRAM caches, for merely hundreds of MBs of stacked DRAM capacity has driven the decision of metadata placement alongside the data in the DRAM devices. These designs have focused on improving access latencies by removing tag-serialization overhead using overlapped tag lookups or storing data in TAD units. Further, to improve hit and miss latencies researchers have proposed mechanisms like tag cache/way-locators and predictors to start early fetch requests based on cache line residency respectively. Providing associativity is known to improve cache hit rates by reducing conflict misses. However in DRAMCaches where tag is stored in DRAM,  associativity comes at a cost. Hit latencies increase due to the tag requiring to be burst out of the stacked DRAM. Hence there is an implicit trade-off between providing better hit-rates and reduced access latency. A higher associativity design is suitable for a GPGPU processor which can trade increased access latency for higher hit rates and use the larger bandwidth of the DRAMCache. On the other hand, a CPU would suffer when using such a design but would benefit from a latency-optimized direct-mapped cache \cite{alloy}. However there is no single DRAMCache organization that fits all the design goals. 
\par The interference caused due to CPU-GPU co-running impairs the performance of the latency sensitive CPU application heavily as compared to the performance loss by the GPU. The CPU looses x\% performance while the GPU drop merely by y\% compared to when they run alone. This suggests that the DRAMCache should be organized to improve the lost CPU performance without compromising the GPU performance. Moreover GPUs context switching capability allows it to be more latency tolerant.  We begin by organizing the DRAMCache as a aggressive latency optimized  Alloy cache. Intuitively, we apply the MAP-I prediction to CPU requests to start early memory access for a miss but allow GPU requests to proceed serially through the cache. However, Figure x suggests, adding a DRAMCache naively still leaves 2X performance between onlyCPU with DRAMCache and that obtained in a IHS architecture. Meanwhile the GPU comes within 8\% of its performance with DRAMCache. Although we cannot reach the performance of only CPU or onlyGPU with DRAMCache there is still enough headroom to bridge the performance.

\begin{figure*}[htbp]
   \includegraphics[scale=1.0]{graphs/motivation-cpu}
   \label{fig:template}
\end{figure*}
\begin{figure*}[htbp]
   \includegraphics[scale=1.0]{graphs/motivation-gpu}
   \label{fig:template}
\end{figure*}
