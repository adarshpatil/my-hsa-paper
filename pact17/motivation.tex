\section{DRAMCache Design for IHS} \label{motivation}
As seen in section \ref{introduction}, the large capacity provided by the stacked DRAM are in line with the working set requirements of IHS processors. Using this capacity as a hardware managed cache could provide performance gains without any application modifications. The bandwidth benefits and modestly improved latency provided by the DRAMCache help improve performance of IHS processors over on-chip SRAM caches of reasonable sizes \cite{amd-exascale1}. However, the organization of the DRAMCache for this type of heterogeneous processors is not immediately intuitive.
\par \textbf{Effect of interference:} The effect of co-running impairs the performance of the latency sensitive CPU application heavily as compared to the performance loss by the GPU. In Figure \ref{fig:motivation} we plot the ANTT improvement normalized to a baseline of CPU+GPU without a stacked DRAM.  The CPU looses on an average of 3.2X performance (maximum drop of 18.6X) while the GPU drops merely by 7\% compared to when they run separately. This suggests that the DRAMCache should be organized to improve the lost CPU performance without compromising the GPU performance. Moreover GPUs capability to context switching warps make it more latency tolerant.
\par \textbf{Metadata overhead:} The large storage requirement for metadata in DRAMCaches - of the order of the last level SRAM caches, for merely hundreds of MBs of stacked DRAM capacity has driven DRAMCache designs to either use larger block sizes (e.g. page size allocation \cite{footprint}) to reduce metadata overhead or co-locate metadata alongside data in DRAMCache \cite{loh-hill,alloy,atcache}. In the former case, misses waste precious off-chip bandwidth in the absence of spatial locality while the latter design faces tag-serialization. Tags-in-DRAM designs have further focused on improving access latencies by removing tag-serialization overhead using overlapped tag lookups \cite{loh-hill} or storing Tag-and-Data units \cite{alloy}. These designs come close to tags-in-SRAM like access latency without concerns of spatial locality characteristics of large block sizes. Hence, \cachename organizes data at 128 byte block size and stores data in DRAMCache in a cohesive TAD unit.
\par \textbf{Associativity:} Providing set associativity is known to improve cache hit rates by reducing conflict misses. In DRAMCaches where tag is stored in DRAM,  associativity comes at a cost. Hit latencies increase due to the tag requiring to be burst out of the stacked DRAM. Hence there is an implicit trade-off between providing better hit-rates and reduced access latency. A higher associativity design is suitable for a GPGPU processor which can trade increased access latency for higher hit rates to make better use the larger bandwidth of the DRAMCache. On the other hand, a CPU would suffer when using such a design but would benefit from a latency-optimized direct-mapped cache \cite{alloy}. Thus, \cachename is organized as a direct mapped cache to achieve better hit times for improved CPU performance. Further, this simplifies the design and eschews the need for tag-caches \cite{atcache} and way locators \cite{bimodal} to improve hit times. Our findings are inline with the commercially adopted stacked MCDRAM on the Knights Landing generation of the Intel XeonPhi processor \cite{xeonphi} where the DRAMCache is organized as a direct mapped cache.
\par \textbf{Miss Penalty:} The access latency reduction provided by a stacked DRAM are marginal compared to the off-chip DRAMs and misses incur an almost double DRAM access latency. To overcome this, researchers have proposed cache line hit predictors \cite{loh-hill,alloy} which are critical to extract performance from DRAMCaches. These predictors start an early access to memory on the probability that the block will miss in the DRAMCache. Intuitively, we apply the MAP-I prediction \cite{alloy} to CPU requests to start early memory access when an access is predicted to be a miss. GPU requests always proceed serially through the cache after verifying via tag match. This helps (a) reduce the wastage of off-chip bandwidth for mis-predictions for GPU (b) avoid large structures that will be required for making reliable predictions for GPUs which might require correlating warp, thread, and SM IDs.
\par \textbf{Row Buffer Hits (RBH) vs Bank Level Parallelism (BLP):} Staked DRAMs are organized as vaults (channels), layers (ranks) and banks within each layer. Each vault has several TSVs which constitute lanes in a channel. Stacked DRAMs provide large bandwidth by organizing DRAMs as several smaller banks within layers. Given this abundant BLP, should DRAMCaches exploit this parallelism over improved RBH? In other words, the addressing scheme of a DRAMCache be organized as RoCoRaBaCh which distributes the cache line in banks of different ranks as opposed to RoRaBaCoCh which stores cache blocks consecutively in the row of a bank. We plot the performance of an IHS processor for both addressing schemes in figure x. We observe that both the CPU and GPU perform x\% and y\% better using the RBH scheme. Consequently, \cachename is addressed using a RBH friendly addressing scheme.
\par Using the above design decisions, we add a DRAMCache to an IHS processor. The third bar in Figure \ref{fig:motivation} plots the performance for this setup. However, we observe that there is an average of 2.6X gap (maximum of 12.2X) between the performance of an onlyCPU with DRAMCache configuration and that obtained by the CPU in an IHS architecture. Meanwhile the GPU comes within 9\% of its performance with DRAMCache [Figure \ref{fig:motivation}]. Although we cannot reach the performance of only CPU or onlyGPU with DRAMCache due to the presence of cross-processor interference, there is still enough headroom to bridge the performance.
\begin{figure}[htbp]
   \includegraphics[scale=1.0]{graphs/motivation-cpu}
   \includegraphics[scale=1.0]{graphs/motivation-gpu}
   \caption{ANTT Improvement of CPU and GPU}
   \label{fig:motivation}
\end{figure}
