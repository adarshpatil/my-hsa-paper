\section{Introduction}\label{introduction}

% HSA Arch diagram
\input{hsa-arch}
\begin{figure*}[!htb]
    \centering
    \hsacpu
    \caption{Architecture of a Integrated Heterogeneous System}
    \label{hsa-arch}
\end{figure*}


\par The remarkable advances in computing power of the modern microprocessor over the last few decades can predominantly be attributed to Moore's Law and advances in manufacturing technology that has allowed shrinking of transistor sizes. Miniaturization of transistors has allowed addition of specialized on-chip hardware circuitry for acceleration units. A study in 2010 by Koomey et. al \cite{koomey} found that the amount of computation that could be done per unit of energy doubled about every 18 months. However, to reach exascale and beyond requires a thousand fold decrease in energy consumed per flop computed. Graphics processing units (GPUs) have evolved from being fixed-function pipelines and are being used to accelerate data parallel code for general purpose (GP) applications. Compared with multi-core CPUs, GPGPUs offer the potential for better performance at lower energy. Traditionally these discrete processors have had their own independent memory systems. To take advantage of the discrete GPUs, the CPU must copy data to the GPUs memory and back. This data movement is wasteful and expends energy while also adding latency as the transfer happens over a slower PCIe bus. The separate address spaces and complex programming models that need to manage 2 sets of data further impede expansion of the workloads that benefit from GPGPU computing. 
\par Modern processor chips have heterogeneous processors which integrate a lower capacity GPUs on-die allowing for graphics rendering only. In view of the widespread use of the GPU for general purpose applications, processor manufacturers including AMD\cite{amd-apu}, Intel\cite{inteliris}, and NVIDIA\cite{denver} are beginning to allow general purpose OpenCL/CUDA programs to run on their Integrated Heterogeneous System (IHS) platform which were so far restricted to graphics rendering. The HSA Foundation \cite{hsafoundation} was setup to develop and define cross-vendor hardware specifications and software development tools needed to allow application software to better use this IHS architecture. This architecture provides a shared virtual address space making pointer sharing semantics possible between CPU and GPU which simplifies programming. Further a shared physical address space reduces GPU initialization time and enables several high level languages to also take advantage of the parallel processing synergistically with the CPU. Programmers can now write applications that seamlessly integrate CPUs with GPUs while benefiting from best attributes of each. Fine-grain data parallel sections in applications like garbage collection of virtual machines \cite{sumatra} and parallel stream processing like face detection, compression, encryption-decryption etc. can now use the integrated GPGPU to deliver better performance. Recent works have proposed allowing GPUs to invoke traditional operating systems paging mechanisms and hardware MMUs to fault pages. This provides the added benefit to be able to run GPU programs whose dataset sizes are not constrained by the memory size. As IHS platforms gain widespread adoption in HPC systems, improving the performance of these platforms will become of paramount importance in the near future. \cite{apu-exascale,amd-exascale1}
\par In contrast to the processing capabilities of the modern microprocessor, DRAM memory speeds have not kept pace commensurately to serve the increasing demands of the processors. This speed imbalance coupled with a limited growth in pin counts has led to the memory and bandwidth wall \cite{memory-wall,bandwidth-wall} for off-chip DRAM systems which often becomes a performance limiting factor. The advent of die-stacking technology \cite{3d-stacking} provides a way to integrate disparate silicon die of NMOS DRAM chips and CMOS logic chips with better interconnects. The implementation is accomplished either by 3D vertical stacking of DRAM chips using through-silicon vias (TSV) interconnects or horizontally/2.5D stacking on a interposer chip as depicted in Figure \ref{hsa-arch}. This allows the addition of a sizable DRAM chip close to processing cores. The onchip DRAM memory can provide anywhere from a couple of hundreds of megabytes to a few gigabytes of storage at high bandwidths of close to 400GB/s compared to the 90GB/s of DDR4 bandwidth [ref]. The better interconnect also lowers the latency of access by around 20-25\% compared to off-chip memory [ref]. Several recent proposals advocate the use of on-chip DRAM capacity as a hardware managed large last level cache for improving performance of multicore CMPs. In the context of IHS architecture, the stacked DRAM can cater to the large bandwidth requirements of throughput oriented GPUs which exploit high levels of MLP. While the latency oriented CPU applications can benefit from reduced latency of data access from the stacked DRAMCache. This also reduces energy consumed per access for the overall system in line with the goals of IHS.
\par Managing contention for shared DRAMCache in the memory hierarchy of the two heterogeneous processor architectures which have asymmetric sensitivity and demands introduces novel challenges. When a GPU kernel is launched it creates large number of concurrently running threads in lock-step in a SIMD execution model and sends large number of requests into the memory hierarchy, which creates a flood of requests that cause congestion in the memory hierarchy while the GPU is executing. The GPU can also switch execution between groups of threads to hide this memory access latency to further exploit parallelism which exacerbates the problem. This causes bottlenecks in request queues and contention for sets in the DRAMCache thus severely hampering CPU performance. Although the GPU can sustain longer memory latencies, as capabilities of execution units in GPUs of IHS chips increase, threads will be able issue larger number of memory requests, thus thrashing the system throughput. Since the GPGPU will be used intermittently for offloading parallel parts of the program, simply reserving shared resources will lead to idling and under-utilization. Also, biasing the design for throughput or latency will adversely impact the performance of the other processor. This necessitates the need for a careful design of memory system for IHS processors, that can handle the GPGPU bursty behavior as well as service requests for the CPU with a seamless/consistent latency without idling resources while delivering improved system throughput at lower energy.

To summarize, we propose to improve the system performance of IHS processors by adding a large capacity stacked DRAMCache. This first level shared memory capacity between CPU and GPU would be used as a hardware managed cache. Our organization \textit{\cachename} is aware of the asymmetric and contrasting requirements from the heterogeneous processors. \textit{\cachename} attempts to meet CPUs requirement of reduced hit times and lower miss penalties while at the same time improving hit rates for GPU to allow it to better use the DRAMCache bandwidth. In the common case when CPU is running alone \textit{\cachename} is a aggressive direct mapped DRAMCache optimized for hit times and lower miss penalty through hit/miss prediction as in \cite{alloy}. However, when GPU is running the CPU is temporally bypassed to utilize the DRAM memory using a bloom filter to identify cache misses and clean cache blocks. Secondly, The CPU is also prioritized at the DRAMCache to reduce the effect of large waiting times caused due to burst of GPU requests. Thirdly, \cachename forces spatial occupancy control by providing a pseudo associativity for GPUs to improve hit rates while still allowing CPU to obtain some benefits of caching. \cachename achieves this using a lightweight and dynamic scheme which does not impose hard partitions on the cache.

\par To the best of our knowledge, this is the first study on the interactions of IHS with a shared stacked DRAMCache. The rest of the paper is organized as follows. Section \ref{motivation} demonstrates the performance that can be gained by adding a DRAMCache to a IHS processor chip and motivates the need for architecting an asymmetry aware DRAMCache organization. We present the organization and design principles of \cachename and its describe its working in Section \ref{mechanism} . Next, in Section \ref{methodology} we describe the experimental setup and methodology followed by evaluation results in Section \ref{results}. Section \ref{related-work} presents related work in this area and Section \ref{conclusion} concludes the paper.
