\section{\cachename for IHS} \label{design}

\cachename builds on an aggressive direct mapped cache design with tags stored in DRAM as TAD units. \cachename caches lines at 128B granularity. In this section, we rationalize these design decision from the observations.

\subsection{\cachename Design} \label{design}

\par \textbf{Metadata overhead:} The large storage requirement for metadata in DRAMCaches - of the order of hundreds of MBs, has driven DRAMCache designs to either use larger block sizes (e.g. page size allocation - 2/4KB \cite{footprint}) to reduce metadata overhead or co-locate metadata alongside data in DRAMCache \cite{loh-hill,alloy,atcache}. In the former case, misses waste precious off-chip bandwidth in the absence of spatial locality while the latter design faces tag-serialization. To understand the spatial locality characteristics in a IHS configuration, we plot the number of 128 Byte sub-blocks used within a 2KB page size cache block. We observe that out of the 16 sub-blocks less than 4 sub-blocks are touched in X\% of the workloads. 
Tags-in-DRAM designs however have further focused on improving access latencies by removing tag-serialization overhead using overlapped tag lookups \cite{loh-hill} or storing Tag-and-Data units \cite{alloy}. These designs come close to tags-in-SRAM like access latency without concerns of spatial locality characteristics of large block sizes. Hence, \cachename organizes data at 128 byte block size and stores data in DRAMCache in a cohesive TAD unit.

\par \textbf{Associativity:} Providing set associativity is known to improve cache hit rates by reducing conflict misses. In DRAMCaches where tag is stored in DRAM,  associativity comes at a cost. Hit latencies increase due to the tag requiring to be burst out of the stacked DRAM. Hence there is an implicit trade-off between providing better hit-rates and reduced access latency. A higher associativity design is suitable for a GPGPU processor which can trade increased access latency for higher hit rates to make better use the larger bandwidth of the DRAMCache. On the other hand, a CPU would suffer when using such a design but would benefit from a latency-optimized direct-mapped cache \cite{alloy}. Thus, \cachename is organized as a direct mapped cache to achieve better hit times for improved CPU performance. Further, this simplifies the design and eschews the need for tag-caches \cite{atcache} and way locators \cite{bimodal} to improve hit times. \cachename's organization is also inline with the commercially adopted stacked MCDRAM on the Knights Landing generation of the Intel XeonPhi processor \cite{xeonphi} where the DRAMCache is organized as a direct mapped cache.

\par \textbf{Miss Penalty:} The access latency reduction provided by a stacked DRAM is typically in the range of 20\%-25\% compared to off-chip DRAMs Hence, the miss penalty is almost twice of a DRAM access latency, since a miss in the DRAMCache would then accesses the memory serially. To overcome this, researchers have proposed cache line hit predictors \cite{loh-hill,alloy} which are critical to extract performance from DRAMCaches. These predictors start an early access to memory on the probability that the block will miss in the DRAMCache. Intuitively, we apply the MAP-I prediction \cite{alloy} to CPU requests to start early memory access when an access is predicted to be a miss. GPU requests always proceed serially through the cache after verifying via tag match. This helps (a) reduce the wastage of off-chip bandwidth for mis-predictions for GPU (b) avoid large structures that will be required for making reliable predictions for GPUs which might require correlating warp, thread, and SM IDs.

\par \textbf{Row Buffer Hits (RBH) vs Bank Level Parallelism (BLP):} Staked DRAMs are organized as vaults (channels), layers (ranks) and banks within each layer. Each vault has several TSVs which constitute lanes in a channel. Stacked DRAMs provide large bandwidth by organizing DRAMs as several smaller banks within layers. Given this abundant BLP, should DRAMCaches exploit this parallelism over improved RBH? In other words, the addressing scheme of a DRAMCache be organized as RoCoRaBaCh (Row,Column,Rank,Bank,Channel) -- referred to as the BLP-scheme -- which distributes the cache line in banks of different ranks as opposed to RoRaBaCoCh -- referred to as the RBH-scheme -- which stores cache blocks consecutively in the row of a bank. We plot the performance of an IHS processor for both addressing schemes in figure x. We observe that both the CPU and GPU perform x\% and y\% better using the RBH scheme. Consequently, \cachename is addressed using a RBH friendly addressing scheme. 

\par Lastly, as shown in Figure \ref{fig:motivation} the CPU suffers significant performance losses while the GPU is relatively unaffected by co-running, compared to when they run separately. GPUs capability to context switching warps make it more latency tolerant. This suggests that the DRAMCache should be organized to regain the lost CPU performance without compromising the GPU performance.  In the following subsection we propose 3 schemes for achieving this. 