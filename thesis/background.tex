\chapter{Background and Motivation} \label{chap:background}
In this chapter, we first present the necessary background for the rest of work. We then state the terminology used in the rest of the thesis. Lastly, we present the motivation for the work.

\section{Background}
In this section, we describe the necessary basics on which this thesis is based. This includes overview of IHS architectures, the basic structure and working of DRAM devices and lastly the organization of stacked DRAMCaches.
\subsection{IHS Architecture}

\begin{figure}[!htb]
	\centering
	\def\svgwidth{0.9\columnwidth}
	\input{figures/arch.pdf_tex}
	\caption{Architecture of an Integrated Heterogeneous System}
	\label{fig:hsa-arch}
\end{figure}
\par A key feature of IHS is the unified memory model.  Figure \ref{fig:hsa-arch} illustrates a typical IHS architecture. It consists of multiple CPU cores with private L1 and shared L2 caches (shared among CPU cores). Alongside are multiple GPU cores or Compute Units (CUs) with private L1 and shared L2 cache (shared among GPU cores). In addition the IHS architecture features the following components.
\begin{itemize}
	\item Shared page table: IHS allows the CPUs and GPUs to operate in a globally shared address space. Thus, address translations are to be shared across CPU and GPUs. Consequently, TLBs and page table walkers on CPUs and GPUs utilize a single page table in memory.
	\item Coherent caches: IHS architecture provides a fully coherent shared memory model between CPUs and GPUs as opposed to discrete GPUs where the driver, at the request of the programmer, must flush and invalidate the caches at required intervals in order for the CPUs and GPUs to share results. In IHS architecture, the cache coherence mechanism between the CPU caches and GPU caches allows the processors to read updated values immediately. This greatly simplifies the programming paradigm and reduces the programmer burden. It also enables fine grained offloading and efficient work sharing between the processors. Coherence is achieved using a region based Heterogeneous Coherence protocol similar to the proposal in \cite{hsc-coherence}.
	\item Shared Physical Memory System: In IHS architectures CPU and GPU processors share a single physical address space. Thus, the cores share memory controllers, channels and the physical memory devices themselves. This also avoids having to explicitly copy data to a different GPU memory as in discrete GPUs.
	\item Shared Network-on-Chip (NoC): A common NoC connects the CPU and GPU cores with the caches and the memory controllers.
\end{itemize}
As IHS platforms gain widespread adoption in HPC systems, improving the performance of these platforms will become of paramount importance in the near future. \cite{apu-exascale,amd-exascale1}. 

\subsection{DRAM Fundamentals and Operation} \label{dram-background}
Dynamic Random Access Memory (DRAM) forms the backbone of main memory in modern processors. DRAMs provide relatively large, fast and cost effective storage capacity to be used as main memory. In this section, we first describe the basics and operation of DRAM  devices.

\par A DRAM stores information in a cell, which is a transistor-capacitor pair \cite{dram-book}. A DRAM device organization contains banks where each bank contains a 2D array (grid) of storage cells. A row decoder circuitry decodes the row address and activates the transistors of the corresponding row, thus reading the data out of the coupled transistors into data lines (\textit{row activation}). A row of sense amplifiers (called row buffers) is used to sense this charge and hold it temporarily for purpose of transferring the data over the data bus. Subsequent requests to columns in this activated row are served from the row buffer. A \textit{column access} command transfers a number of columns (consecutive bytes) to/from the row buffer. Since the read from the DRAM array is destructive i.e., the charge on the capacitor in the cells is lost, a \textit{precharge} operation writes back contents from the sense amplifiers back to the corresponding row.
\begin{figure}[!htb]
	\centering
	\def\svgwidth{\columnwidth}
	\input{figures/dram-basics.pdf_tex}
	\caption{Organization of an DRAM Memory System}
	\label{fig:dram-basics}
\end{figure}
\par In order to improve efficiency of the DRAM systems, DRAM devices are organized hierarchically. Banks are further organized into ranks which share command buses but have separate data buses. A chip select is used to select a bank to issue a command.  The banks in a rank operate in parallel and serve data independently to match the data bus width. A typical memory system would consist of multiple channels with completely independent DRAM devices having separate data and address buses. A DDRx (double data rate) DRAM transfers data on both edges of the bus clock (rising and falling). DDR increases the n-bit prefetch operation in each successive generation of DRAMs from 4n in DDR2 to 8n in DDR3. Here the "8n-prefetch" means that at each cycle a DDR3 DRAM transmits 8 bits of information from internal DRAM banks into the I/O buffers before sending them out. In other words, the minimum number of bits a DRAM column read/write performs in each operation for a DDR3 DRAM is 8 bits. This effectively increases the bandwidth without changing the core of the DRAM.
\par Each operation on the DRAM takes a certain fixed number of cycles. The timing cycles for the primary operations of row activation, column read, row precharge on the DRAM are denoted as $t_{RCD}$ , $t_{CL}$ and $t_{PRE}$. There are also several other timing parameters of DRAM circuits that also need to be respected during the operation of DRAMs due to the electrical constraints imposed by them. For example, $t_{WR}$: write recovery time for the written data to propagate into the DRAM arrays, $t_{WTR}$:write-to-read turnaround time, $t_{CCD}$: minimum column-to-column command spacing to be adhered, $t_{FAW}$: a rolling time-frame in which a maximum of four row activations on the same DRAM device can be engaged, $t_{RRD}$: a minimum row-to-row activation spacing to be maintained between two row activations and several others as specified by JEDEC \cite{jedec-ddr3}. The above timing constrains are complicated by the "dynamic" nature of these devices i.e., the transistors storing the charge in the DRAM are not perfect and leak charge, requiring periodic refresh. This also limits the read and write operations that can be performed on DRAM devices.
Further, two widely accepted factors that impact DRAM memory system performance are
\begin{itemize}
	\item Row Buffer Hit (RBH): When a column access is performed to a row which is currently in the row buffer, a single column access is required to retrieve data. Thus, such an access would incur a single $t_{CL}$ access latency without requiring any row operations. This scenario is referred to as a Row buffer hit and the corresponding parameter for a stream of accesses is known as the row buffer hit rate. Incurring row buffer hit would reduce the service time for the request. 
	\item Bank Level Parallelism (BLP): Requests to independent banks can proceed in parallel and thus the temporal overlap can help hide the additional latencies incurred. Thus, if the requests are spread over banks the memory access latency can be amortized using this temporal overlap.
\end{itemize}
\par To achieve best efficiency from the DRAM requires careful scheduling of requests to extract maximum parallelism while respecting the timing constrains of the DRAM. A memory controller controls and coordinates the operation of the DRAM device while respecting timing constrains and bus conflicts. The memory access scheduler, picks a request to service from read or write queues. Reordering requests allows memory controller to schedule requests to available banks while other requests wait for access. However, reordering does not compromise correctness as requests to the same address are coalesced but performed in the order that the accesses were received. For the memory access scheduler, performance is determined by the following parameters
\begin{itemize}
	\item Waiting time: Time spent by the request waiting in the queue.
	\item Service time: Time taken by the request at the head of the queue to be serviced.
\end{itemize}
A popular DRAM access scheduler is the first-ready, first-come-first-service (FR-FCFS) scheduler \cite{frfcfs}. The FR-FCFS scheduler prioritizes requests that will be a hit in the open row of DRAM banks over other requests i.e., row buffer hit requests are prioritized. If no request is a row-buffer hit, then FR-FCFS prioritizes older requests over younger ones.

\par Commercially, DRAMs are available as DIMMs (Dual in-line memory module) containing several DRAM devices with a standard interface. In this work, we model a JEDEC DDR3 DRAM \cite{jedec-ddr3} as the off-chip memory device.
\par Subsequently, in Chapter \ref{chap:hashcache}, we revisit these DRAM performance parameters to evaluate suitable DRAM addressing schemes for the new family of die stacked DRAMs. We also develop a DRAM memory access scheduler that factors in the idiosyncrasy of the requesting processor.

\subsection{Stacked DRAM as Hardware Managed Cache} \label{dramcache-background}
As alluded to earlier, DRAMCaches require careful design to be effective. Traditional SRAM caches provide capacities of upto few tens of megabytes, have fast access times but expend higher energy and also have higher circuit costs. DRAMCaches on the other hand, provide much larger capacity (several hundred megabytes to a few gigabytes) at lower power and higher density. However, they are constrained by access times. 
Logically, the working of DRAMCaches is very similar to that of SRAM caches. Thus, cache design parameters like cache block size, set-associativity, miss penalty etc. remain pertinent parameters even in DRAMCaches.
%When a request arrives for the data that is currently unavailable in the cache, the cache controllers request for a fixed size block or chunk of data from the memory or other caches behind it. The data is then resident in the cache until it is evicted by another request. 
However, some design decisions require to be re-examined in context of the DRAMCache characteristics.
\par Due to large capacity, DRAMCaches have a large amount of metadata. This metadata includes tags, valid, dirty and replacement bits. This metadata size is of the order of megabytes even for reasonably sized DRAMCaches. To illustrate this problem, consider a 256MB DRAMCache with 128B cache block size and with metadata of 8B per cache block. This DRAMCache organization incurs a metadata overhead of 16MB. Even with large granularity cache block size, as DRAMCache capacities increase, metadata continues to incur similar overheads. Considering a 1GB DRAMCache organized at 1KB cache block size still incurs a metadata overhead of 8MB.
\subsubsection{Metadata Management and Access Latencies}
\begin{figure}[!htb]
	\centering
	\def\svgwidth{\columnwidth}
	\input{figures/dramcache-lat.pdf_tex}
	\caption{Access Latencies for Various Organizations of DRAMCache assuming a Tag Hit}
	\label{fig:dramcache-lat}
\end{figure}
Metadata management is critical in caches as this is in the critical path of all cache requests (hit or miss). There have been 2 design approaches to tackle the metadata issue. With the running example of a cache block hit we explain the latencies incurred by each approach and accordingly Figure \ref{fig:dramcache-lat} shows the timeline and sequence of events for each approach. The corresponding operation for a cache block miss would only perform the tag retrieval without the data retrieval. The first proposal called \textit{Tags-in-SRAM} proposes to store metadata in SRAMs \cite{footprint}. This proposals allocates blocks at large granularity and stores metadata in SRAM. However, large blocks waste off-chip bandwidth due to fetching of unused data. Nevertheless, metadata and tag lookup is fairy fast due to SRAM structure employed as shown in figure \ref{fig:dramcache-lat}(a). The second set of proposals called \textit{Tags-in-DRAM} \cite{loh-hill,alloy,bimodal,atcache} store metadata within the DRAMCache. This incurs no additional SRAM costs but are subject to higher access latency since the tags have to be read from the DRAMCache. Figure \ref{fig:dramcache-lat}(b) shows the access latencies for a naive \textit{Tags-in-DRAM} implementation. Further, a \textit{Tags-in-DRAM} organization can be optimized by placing tag and data of a set in the same row of the DRAMCache. This organization, called compound access \cite{loh-hill}, allows the subsequent data retrieval burst after a tag match to be a row buffer hit as shown in Figure \ref{fig:dramcache-lat}(c). A small metadata cache \cite{atcache} or predictor \cite{loh-hill} structure is usually employed for \textit{Tags-in-DRAM} organization to reduce DRAMCache lookups for tags.
\subsubsection{Alloy Cache} \label{alloy-background}
\begin{figure}[!htb]
	\centering
	\def\svgwidth{\columnwidth}
	\input{figures/dramcache-rb.pdf_tex}
	\caption{Row Buffer Layout of Alloy Cache adapted for Cache Line size of 128 Bytes}
	\label{fig:dramcache-rb}
\end{figure}
\par One particular proposal, we detail here is the organization of Alloy Cache proposed by Qureshi and Loh in \cite{alloy}. Alloy Cache was designed and evaluated for multicore CPUs and is a latency optimized \textit{Tags-in-DRAM} design. Alloy Cache allocates blocks at small cache block granularity (128 bytes) and stores metadata for the blocks within the DRAMCache. However it is able to provide close to SRAM-like access latency to metadata by \textit{alloying} the cache and data into blocks called TADs (Tag-and-Data). The cache is organized as a direct mapped DRAMCache and stores 15 sets in a DRAMCache row of size 2KB as shown in figure \ref{fig:dramcache-rb}. As a result some negligible capacity (8 bytes) of the DRAMCache row is unused. When a cache line is probed, Alloy Cache uses a larger burst size to retrieve the 136B TAD (128B data + 8B metadata) in a single DRAMCache access. Thus, a cache hit/miss decision can be made when the TAD is retrieved. In case of a hit, the data retrieved is used else the data is discarded. The Alloy Cache achieves access latency for the DRAMCache as shown in figure \ref{fig:dramcache-lat}(d). To further optimize the total observed memory access time, the Alloy Cache incorporates a Memory Access Predictor, called \textit{MAP-I} predictor for short, which is based on the address of the L2 miss causing instruction. The MAP-I predictor uses an array of counters hashed with the folded-xor of the program counter to predict a DRAMCache hit/miss. They observe that such a predictor shows a good correlation between DRAMCache hit/miss information and the instruction address. MAP-I achieves a predictor accuracy of close to 95\% at a cost of metadata overhead of a few bytes. Since a predictor is not perfect, they use this prediction information to start early access to off-chip memory when the predictor predicts a miss for an access to the DRAMCache. This is referred to as the Parallel-Access-Model (PAM) and this enables Alloy Cache to hide the miss latency for DRAMCache by overlapping the off-chip memory access time with the DRAMCache access time.

\section{Terminology and Keywords}
We describe some of the terminology used in the rest of this thesis.
\begin{itemize}
	\item IHS - Integrated Heterogeneous Systems - refers to the platforms which have an GPGPU integrated on chip alongside the multi-core CPU. As shown in Figure \ref{fig:hsa-arch}, these processors have a cache coherent interconnect between the CPU and GPU and share a unified physical memory.
	\item HeA - Heterogeneous Applications - refers to the workload where multiple single threaded CPU benchmarks and a single GPGPU application are co-run in a IHS architecture.
	\item HoA - Homogeneous Applications - contextually refers to either (multiple single threaded) CPU benchmarks that run on a multi-core CPU architecture (HoA CPU) or a single GPGPU application run alone on one CPU and multiple GPU SMs (HoA GPU).
	\item DRAMCache - vertically die stacked DRAM, used as a hardware managed cache.
	\item D\$ - Refers to the die stacked DRAMCache as discussed above (disambiguation against data cache).
	\item \cachename\ - Heterogeneity Aware Shared DRAMCache for Integrated Heterogeneous System - Refers to the set of all designs and mechanisms that constitute our DRAMCache.
\end{itemize}
We refer to OpenCL \cite{opencl} and CUDA \cite{cuda} terminology interchangeably in the rest of this document.



\section{Motivation} \label{motivation}
The IHS architecture integrates two different classes of compute devices i.e., latency oriented CPU and throughput oriented GPUs on a single chip sharing a physical memory. In this section, we motivate the need for better memory class devices that can cater to the requirements of IHS architectures. Specifically, we show that the die stacked class of DRAMs are a good fit for IHS architecture since they provide the necessary bandwidth and latency capabilities to deliver data to the processing units. However, managing contention in these DRAMs for a heterogeneous processor architecture which have asymmetric sensitivity and demands introduces novel challenges.

\subsection{Memory Subsystem in IHS Architecture}
We first analyse the impact of CPU cores and GPU cors co-running in an IHS architecture. 
We use the gem5-gpu \cite{gem5-gpu} simulator for this exercise. We follow experimental methodology as described in Section \ref{simulation-methodology}. The IHS has 5 CPU cores with private per CPU L1 caches (32KB I/D split) and CPU shared L2 cache (1MB) along with 8 GPU SMs with per SM private L1 cache (64KB) and GPU shared L2 cache (512KB).
The shared off-chip memory is modelled after a traditional DDR3-1600 like memory, similar to the one in modern multi-core systems. 
\par Figure \ref{results-interference} shows the performance of (a)CPU and (b)GPU in terms of Harmonic Mean of IPC for HoA and HeA applications running on an IHS without a DRAMCache. We observe that the HoA CPU applications perform 3.19x better than HeA applications in an IHS architecture. On the other hand, the performance HoA GPU applications is only marginally better by 7\% compared to the on-chip GPU running HeA applications in the IHS. This shows that applications running in an IHS suffer performance losses as compared when they are run homogeneously. The loss in performance is more significant for the CPU applications as compared to the GPU applications in an IHS. 
\par When a GPU kernel is launched it creates large number of concurrent threads which run in lock-step SIMD execution model and sends a large number of requests into the memory hierarchy. The memory system does not provide sufficient bandwidth to cater to these large number of requests, causing congestion and severely hampering CPU performance. Further, GPUs are designed to tolerate longer memory latencies while the memory hierarchy of typical CPUs have been designed to optimize memory access latencies for CPUs. The GPUs hide the latency of memory access using interleaved scheduling of hundreds of concurrently running threads while CPU designs have a limited size reorder buffers (typically 128 entries) which is exhausted quickly leading to the processor stalling. This jeopardizes the overall performance of the IHS, quickly diminishing the benefits provided by integration of the on-chip GPU. This motivates the need for better memory devices that can cater to the requirements of IHS architectures.
\par We note here that similar observations regarding the performance reduction of co-running CPUs and GPUs in IHS as compared to their HoA performance have been made earlier in the works of \cite{helm,oscar}. However, these works focus exclusively on issues of shared SRAM cache management, network-on-chip management and non-volatile STT-RAM cache management for IHS architectures.

\begin{figure}[htb]
	\centering
	\includegraphics{graphs/results-cpu-nodc}
	\includegraphics{graphs/results-gpu-nodc}
	\caption{Effect of Interference in the IHS architecture due to the co-running on (a) CPU (b) GPU}
	\label{results-interference}
\end{figure}

\par Large computational capabilities of the IHS architecture require proportionately capable and aggressive memory system to deliver data to the computation engines as demanded. Current DRAM memory systems like DDR3, DDR4 etc. are designed for multi-core CPUs and provide small number of channels (typically 2 to 4), yielding a total memory bandwidth of a few GB/s. On the other hand, on-chip SRAM caches can only provide capacities of a few tens of MBs which will not be able to contain the working set requirements of IHS architectures. Even with non-volatile STT-RAM caches the device characteristics introduce challenges in the operation and the die sizes restrict the capacities that they can provide. Hence, we infer that the memory hierarchy needs be revisited to support IHS architectures effectively.


\subsection{Addition of Die-Stacked DRAMCache to IHS}
To avoid the performance degradation due to co-running in IHS we propose addition of a large capacity die stacked DRAM, used as a hardware managed cache. The immense bandwidth provided by these DRAM class devices can improve the performance of throughput-oriented GPU while the reduced latency of access can assist in improving the performance latency-oriented CPUs. The large capacity provided by the stacked DRAM is in line with the working set requirements of IHS processors. Using this capacity as a hardware managed cache could provide performance gains without any application modifications. The bandwidth benefits and modestly improved latency provided by the DRAMCache help improve performance of IHS processors. 
\par To design an effective DRAMCache for IHS requires balancing conflicting set of design parameters. A DRAMCache design that offers lower hit-time and hence a lower miss penalty for the previous level caches (L2 cache) would thus benefit the CPUs. While on the other hand, the GPU cores would benefit from higher bandwidth and higher hit-rates in DRAMCache even at the cost of higher hit-time. This necessitates the need for a careful design of the DRAMCache for IHS processors, that can handle the GPGPU bursty behaviour as well as service requests for the CPU with a consistent latency without idling resources while delivering improved system throughput at lower energy.

\par In this work, we assume a cache organization similar to Alloy cache \cite{alloy} with a block size of 128 bytes and study the problems and challenges in using the DRAMCache for IHS architectures. We justify these design decisions in the Chapter \ref{chap:hashcache}. 
Each CPU core has a private L1 cache and a shared split L2 cache across the CPU cores. The GPU cores have private L1 and shared L2 cache among themselves.  
The stacked DRAMCache (of size 64MB) is the first level shared cache across CPU cores and GPU cores. In our experiments, we consider a multi-programmed workload on the CPU cores and a GPGPU application on a single CPU core and multiple CUs 
\footnote{It should be noted here that the GPU application has a CPU component and alternates execution on CPU core and the CUs (kernel execution)}. 
%We refer to this workload as Heterogeneous Application (HeA), as the CPU and GPU applications are co-run. We use the terms Homogeneous Application (HoA-CPU and HoA-GPU) to refer to the cases when (multiple) CPU applications are run alone and GPU application is run alone. 
%Our study on the interference due to co-running CPU and GPU application reports performance numbers of the GPU application when the GPU kernel is running on the CUs.
Additional details relating to the experimental methodology and workloads are described in Chapter \ref{chap:simulator}


\begin{figure}[htbp]
	\centering
	\includegraphics[scale=1]{graphs/motivation-cpu}
	\includegraphics[scale=1]{graphs/motivation-gpu}
	\caption{Performance Comparison of (a) CPU (b) GPU when running alone, co-run, with and w/o DRAMCache}
	\label{fig:motivation}
\end{figure}


\par First, we evaluate the benefits of having a large DRAMCache for the CPU in an IHS architecture. Figure \ref{fig:motivation}(a) presents the performance improvement for CPU cores due to the addition of a stacked DRAMCache (in terms of harmonic mean of IPCs, normalized to HeA without a DRAMCache) when co-run with a GPU and when run alone.
% that is obtained by adding a stacked DRAMCache when co-run with the GPU application and also when run alone. 
As can be seen from the figure, the addition of a stacked DRAMCache improves performance of CPU applications in an IHS architecture, on an average by just 42\%. However, when the CPU runs alone with a stacked DRAMCache it achieves a 3.72x better performance than that achieved in IHS without a DRAMCache. 
%In other words, the performance gain by adding a DRAMCache is on an average 2.6x lesser in IHS. 
%We observe that this huge performance gap is primarily due to the interference of GPU applications.
Although this HoA-CPU performance cannot be achieved with co-running applications in IHS (interference effects cannot be removed), there still exists a significant opportunity for improvement.
This gap in performance can be attributed to the unmanaged heterogeneity and interference in the DRAMCache organization.

\begin{figure}[htb]
	\centering
	\includegraphics[scale=1]{graphs/motivation-cpu-cache}
	\caption{DRAMCache CPU Access Latency and Hit rate}
	\label{fig:motivation-cpu-cache}
\end{figure}

\par We further investigate the cause of this performance gap. Figures \ref{fig:motivation-cpu-cache} presents the DRAMCache access latency experienced by a CPU request (in terms of CPU cycles) on the primary (left) y-axis and the CPU hit rates on the secondary (right) y-axis, while running alone and co-run with the GPU application. We find that the presence of GPU application increases the average access latency of the DRAMCache by 113\% while hit rates of the CPU are marginally impacted (only by about 4\%) when co-running.
This increase in average access latency is primarily attributed to the large
number of GPU requests flooding the DRAMCache controller when the GPU kernels are co-running with the CPU applications.
It should be noted here that, even though we use the terms CPU and GPU requests separately, they may refer to the same data. This terminology only indicates the source of the request. 
%We also note that the GPGPU application has some amount of sharing of data within the DRAMCache between the CPU and GPU processors (i.e. constructive interference - blocks brought in by the CPU and then used by the GPU and vice versa).
\par Next, we study the impact of co-running applications on the GPU cores. Figure \ref{fig:motivation}(b) presents GPU performance obtained by addition of a stacked DRAMCache.
We observe that with the introduction of DRAMCache in an IHS, the GPU performance improves on an average by 24\%. This performance is within 10\% of its HoA GPU performance with DRAMCache when the GPU application is cor-un with CPU applications (HeA) and without them (HoA).
Thus, co-running with CPU applications has only a minor impact on the GPU performance. 
%Some cache-sensitive GPU workloads show benefits from the addition of the DRAMCache while other workloads, that do not significantly reuse cached data, show no performance benefits.
%\begin{figure}[htb]
%   \centering
%   \includegraphics[scale=0.8]{graphs/motivation-gpu-cache}
%   \caption{GPU Miss reduction with 2-way associativity}
%   \label{fig:motivation-gpu-cache}
%\end{figure}
%\par Additionally, we observe that providing associativity in DRAMCache reduces the number of GPU misses going to the relatively constricted off-chip DRAM bus. 
%Figure \ref{fig:motivation-gpu-cache} plots the reduction in number of GPU misses in log scale, by introducing 2-way associativity over our direct mapped DRAMCache. The GPU misses reduce by an average of $10^5$ for 2-way associative cache with half the number of sets and $10^6$ for a 2-way associative cache with the same number of sets.
%In comparison, the total number of CPU read requests to the DRAM are in the order of $10^7$. This considerable reduction in GPU misses leads to lower congestion for CPU requests at the DRAM. While associativity leads to only a modest improvements in hit rates for the GPU, it avoids incurring double queuing latency for all the reduced GPU misses. This corroborates the observation by other works \cite{oscar} that the number of requests from the GPU are orders of magnitude higher than CPU.
%Thus, providing some associativity can increase DRAMCache bandwidth utilization and improve the system performance.

While the above discussion indicate that the design decisions of heterogeneity aware DRAMCache are influenced by the latency-sensitive CPU applications, we emphasize the need to effectively utilize the large capacity and the higher bandwidth of DRAMCache by GPU applications. Towards this goal, we experimented with a DRAMCache of 128MB and 2-way associativity. We observe that the hit rate for GPU applications improves, on an average, by 5\%. While the improvement in hit-rate is not significant, as the number of GPU requests is very large (2 or 3 orders of magnitude higher than the CPU requests), even such a small increase in the hit rate leads to large improvements in utilization of the DRAMCache bandwidth by the GPU application. This makes a case for improving the associativity for GPU requests by introducing some pseudo-associativity in the DRAMCache, without impacting the hit-time offered by a direct-mapped tag-and-data (TAD) cache.

\par Based on the above motivation study we conclude that (i) there is significant performance impact of co-running of CPUs and GPUs in IHS architectures with the standard DDRx like memory interfaces (ii) there are significant performance benefits that could be obtained 
with the introduction of the stacked DRAMCache for the latency-sensitive CPU applications. However, in a naive implementation
of the DRAMCache, these benefits can be lost due to interference from the co-running GPU application.  Hence it is important to carefully 
architect the DRAMCache organization to ensure CPU applications are not hampered due to this interference.
This requires the design to be aware of the heterogeneity of the applications (CPU vs GPU) and their demands on the 
memory hierarchy.  Hard partitioning of DRAMCache would not be an effective design as it leads to under-utilization of the large capacity 
stacked DRAM, as the effects of interference due to co-running GPU application is felt only when the kernel is run on the GPU sporadically. 
Further, effectively utilizing the under-utilized main memory bandwidth \cite{micro-refresh, mainak-hpca, bear}
is also important to achieve higher performance.  Lastly, the design should ensure that the CPU applications and the 
GPU application are able to utilize the large capacity of the stacked DRAM effectively, to meet the working set 
requirements of the respective applications in the best possible way.

\section{Summary}
In this chapter, we presented the basics of IHS architecture, DRAM memory system and the DRAMCache organizations which form the background for this work. We then motivated the need for better memory systems for supporting IHS architecture. We showed that the stacked DRAMCache device characteristics hold the potential to improve performance of IHS processors but in a naive implementation these benefits are negated.