\chapter{Simulation Infrastructure} \label{chap:simulator}
In this chapter, we articulate the simulator setup and enhancements done to simulate the IHS architecture with the shared stacked DRAM cache. We then outline the experimental methodology, benchmarks comprising of composite heterogeneous workloads used in our evaluation and the parameters used to report system performance in our evaluation.

\section{The gem5-gpu Simulator} \label{gem5-gpu-simulator}
The gem5-gpu \cite{gem5-gpu} is a heterogeneous cycle accurate CPU-GPU simulator used to simulate a hybrid system with both CPU and GPU types of cores. It integrates gem5 \cite{gem5} for the multi-core CPU simulation and gpgpu-sim \cite{gpgpu-sim} for simulating the compute units of a general purpose GPU with a flexible and configurable memory hierarchy using gem5's Ruby memory system \cite{gems}. It can model a system with multi-core CPUs and a discrete GPU with a "split" memory hierarchy or an integrated heterogeneous system with tightly coupled CPU and GPU cores with a "shared" cache coherent interconnect. We refer to the gem5-gpu from here on running an IHS architecture with a shared virtual and physical address spaces as described in Chapter \ref{chap:introduction}. The gem5-gpu can run unmodified operating system and application binaries. The simulator is also performance validated against hardware and is completely open source \cite{gem5-gpu}. For our evaluation we run gem5-gpu in System Call emulation mode with the base operating system performing the CUDA driver emulation.

\subsection{Addition of Shared 3D Die-Stacked DRAM cache}
The Ruby cache and memory hierarchy in gem5-gpu is authored using the domain specific language SLICC (Specification Language for Implementing Cache Coherence) \cite{gems}. SLICC allows the definition of cache levels and behaviour of the coherence protocol. The directory controller is the memory side coherence engine and participates in the coherence protocol. These directory controllers have message queues to send and receive memory requests from the DRAM Controllers. The DRAM controllers are responsible for the operation of the DRAM devices. These DRAM controllers queue requests and orchestrate memory access scheduling to the DRAM according to the type of memory device (e.g., DDR, LPDDR, GDDR, HMC etc). To study the interference of CPU and GPU traffic we model the DRAM cache as the first shared level of cache between the heterogeneous CPUs and GPUs while the L2 SRAM caches are shared between homogeneous CPU cores and GPU cores. We modified the IHS architecture to include a shared split L2 cache shared across multi-core CPUs instead of the default per CPU private L2 cache. For this we modified the existing VI\_Hammer coherence protocol \cite{gem5-gpu} to simulate the shared L2 Cache.
\begin{figure}[!htb]
	\centering
	\def\svgwidth{\columnwidth}
	\input{figures/simulator-memory.pdf_tex}
	\caption{Memory Hierarchy as Modelled in Simulator}
	\label{fig:simulator-memory}
\end{figure}

\par The DRAM cache we evaluate here is a memory-side \cite{skylake,mainak-hpca} shared cache between the 2 split cache hierarchies of CPUs and GPUs. Memory-side caches are ones that are outside the coherence domain as shown in Figure \ref{fig:simulator-memory}. They are logically just in front of the memory and serve to increase the memory's effective bandwidth and reduce the average latency of memory accesses. It does not introduce any new states to the directory in the coherence protocol i.e., there is no state called dirty in DRAM cache, or there is no need to snoop it separately and a single memory state is enough to serve data from it. Further, access to memory is interleaved across multiple memory controllers. To avoid a biased interleaving due to uniformly strided address patterns, in our simulation we employ a basic {\tt XOR}-based address hashing mechanism. Each memory controller manages a 4GB DDR3 memory device and a 32MB stacked DRAM vault. The stacked DRAM vault caches data from the corresponding 4GB memory device that the controller is responsible for. This setup ensures that there are no cross bus requests between controllers. Since our simulation setup has the limitations of having a 1-to-1 channel mapping between a stacked DRAM vault and a off-chip DRAM channel, our model does not provide sufficient channel level parallelism as is essential in a stacked DRAM device. To circumvent this limitation we increase the number of layers (ranks) to provide higher amount of parallelism in our stacked DRAM. However, the bank capacity is retained as would be in a standard stacked DRAM. The DRAM cache is non-inclusive  \cite{coherence-dramcache} of the L2 caches above it and uses a write no-allocate policy. 

\subsection{DRAM cache Controller Design}
The DRAM cache Controller consists of several constituent components as shown in Figure \ref{fig:dramcache-ctrl}. The highlighted components (gray fill) in the figure correspond to the components that are unique/exclusive to a DRAM cache controller and are not present in a conventional DRAM controller. These include fill queue, MSHR and WriteBuffer.

\begin{figure}[!htb]
	\centering
	\def\svgwidth{\columnwidth}
	\input{figures/dramcache-ctrl.pdf_tex}
	\caption{DRAM cache Controller Components}
	\label{fig:dramcache-ctrl}
\end{figure}

\par DRAM cache controllers similar to conventional off-chip DRAM controllers consists of request queues. Each request in the queue corresponds to the address of a single burst of data from the stacked DRAM device.
In our design, cache lines are stored as TAD units (tag-and-data) \cite{alloy} and each TAD is divided into 2 bursts, where each burst corresponds to the width of the data bus of the stacked DRAM.
Conventional DRAM controllers enqueue requests into a read or a write queue depending on the type of request. 
However, more subtly unlike conventional DRAM devices, each write request in the DRAM cache entails a tag read and match followed by a write to the data of the TAD unit only if the tag matches (i.e., request is a hit). Thus, both read and write requests in a DRAM cache perform a DRAM read operation. To avoid complicating the model and implementation, we choose to hold all write requests, irrespective of hit or miss, in the same write queue. This allows us to apply uniform priority to all write requests as these are not in the critical path.
Our stacked DRAM cache also models a third type of queue called a Fill Queue \cite{dca} which enqueues requests to insert data into the cache when the requests return from main memory. A fill request performs a DRAM write operation only. 

\par We also model MSHR and WriteBuffers with their associated latencies to realize the precise working of caches. An MSHR is allocated on a cache line miss in the DRAM cache and subsequent requests to that cache line are coalesced and added as targets in the MSHR. A single fetch request is sent to memory corresponding to an MSHR. Once the memory request returns, a response with the data is sent for each target within the MSHR before it is deallocated. Subsequently, a fill request is also created to insert the TAD back into the DRAM cache. 
A WriteBuffer is allocated when (a) a dirty cache line is evicted from the DRAM cache, thus requiring a write back to memory (b) a write request misses in the DRAM cache. Since the DRAM cache is designed with a write no-allocate policy, write misses in DRAM cache are directly forwarded to the memory. The WriteBuffer is deallocated when the write to memory returns and an acknowledgement is sent to the requester. 
\par When scheduling a request to the DRAM cache, the control logic first selects a queue. The DRAM cache control logic prioritizes reads over writes as reads are on the critical path. This is done by allowing the write queue to fill up to a certain high threshold (\textit{write\_high\_threshold}) before switching the bus for writes. Switching the bus from read to write or vice versa incurs a delay and hence a minimum number of writes (\textit{min\_writes\_per\_switch}) are performed before switching the bus back to servicing read requests. If the read queue is empty, we avoid starving the writes and prevent the device from being underutilized/idle by allowing the bus to switch to writes if the write queue has a certain minimum number of requests (\textit{write\_low\_threshold}). The DRAM cache control logic considers the fill queue as part of the write queue thresholds and switches to servicing a fill queue request in the write mode, only when a fill queue high threshold is reached (\textit{fill\_high\_threshold}). Once the queue is picked, the DRAM cache scheduler then picks a request to be serviced from the queue according to the scheduling algorithm. The address of the request is decoded according to the  address mapping policy and appropriate address and commands are sent over the bus while respecting the device timings and refresh constraints.
\par To simplify data management and ensure correctness we choose not to implement a backing store for the DRAM cache and instead share the backing store instance of the off-chip DRAM itself.

\subsubsection{Reject-Retry Mechanism} Interactions between the components, for example DRAM cache Controller and off-chip DRAM controller, is implemented as a master-slave port architecture. A master port sends requests while the slave port receives requests and invokes appropriate functions to act on the requests. For example, when the directory controller's memory master port sends a request to the slave port of DRAM cache Controller it enqueues the request in the appropriate queue. Similarly, a slave port sends out responses and the master port receives these responses and performs appropriate action. For example, when the DRAM controller responds to a read request, the DRAM cache controller clears the corresponding MSHR and sends a response to the requester.
\par However, a cache can be in a blocked state when MSHRs or targets within a MSHR or WriteBuffers are unavailable. At such a time, when a request is received at the DRAM cache slave port but is not able to act on it due to the cache being blocked or read/write queue being full or due to mechanism restrictions as in \prioname, the request is rejected and the reason for the blocking is remembered (i.e., the resource causing the block). When the corresponding resource becomes available, a retry message is sent to the master port from the slave port. Once the master port receives this message, the request is resent. This is called the reject-retry mechanism. This mechanism is independent of the network-on-chip that handles routing, channel partitioning etc.

\subsection{System Configuration} 

\begin{table}[h]
	\centering
	\begin{tabular}{{@{}ll@{}}}
		\toprule
		CPU Core 	& five 4-wide out-of-order x86 cores @2.5 GHz \\
		\midrule
		CPU Caches 	& 32KB 8-way split I/D private L1 Cache, \\ 
		& 1MB 8-way shared split L2 Cache, 128B lines \\
		\midrule
		GPU Core 	& eight Fermi SMs@700MHz, 2x2 GTO warp sched\\
		& upto 32 threadblocks, 64 warps of 32 threads, \\
		& 64K registers, 96KB scratch memory \\
		\midrule
		GPU Caches 	& 64KB 4-way private L1 cache,\\ 
		& 512KB, 16-way assoc L2 Coherent Cache \\
		\midrule
		Stacked     & 2 vaults, HMC\_2500\_x64, 2KB Page \\
		DRAM		& $t_{CL}$-$t_{RCD}$-$t_{RP}$-$t_{RAS}$=9.9ns-10.2ns-7.7ns-21.6ns\\
		& 8 layers/vault, 4 banks/layer\\
		& 64 byte burst size, Peak bandwidth 40GB/s \\
		& Refresh related: $t_{REFI}$=3.9us $t_{RFC}$=59us \\
		\midrule
		Off-chip 	& 2 channels, DDR3\_1600\_x64, 1KB page \\
		DRAM		& $t_{CL}$-$t_{RCD}$-$t_{RP}$-$t_{RAS}$=13.75ns-13.75ns-13.75ns-35ns\\
		& 1 rank/channel, 8 banks/rank\\
		& 64 byte burst size, Peak bandwidth 25GB/s \\
		& Refresh related: $t_{REFI}$=7.8us $t_{RFC}$=260us \\
		
		\bottomrule
	\end{tabular}
	\caption{Configuration of the Simulated System}
	\label{configuration}
\end{table}

The gem5-gpu is configured to run 5 CPU cores (4-wide out-of-order cores, operating at 2.5GHz) with a 32KB private L1 Cache (split I/D), and a 1MB shared L2 Cache (shared across all CPU cores). The IHS also consists of 8 Fermi-like compute units, operating at 700MHz with a private 64KB L1 and 512KB shared L2 cache (shared across all CUs). The details of the IHS configuration are given in Table \ref{configuration}. The private L1 of CUs are non-inclusive of the shared GPU L2 cache and can hold stale data. However, GPU L2 cache is coherent with all levels of the CPU hierarchy. The CPU caches and GPU L2 cache are kept coherent in the simulator. Table \ref{configuration} provides the simulator setup details. 
Our simulator also respects all significant timing and functional parameters for the stacked DRAM cache (including refresh, data bus, request queues, scheduling algorithms, command signalling and clock frequencies) using the DRAMCtrl \cite{dramctrl} model.

\section{Workloads} 
Our goal is to study the performance impact of the large last level shared DRAM cache in IHS architectures. Hence we run multiple applications in parallel on the multi-cores and the GPGPU. Applications having high and medium memory intensive behaviours from SPEC CPU2006 suite \cite{spec2006} were chosen to form a multi-programmed workload that run completely on the CPU. Table \ref{single-cpu-mpki} classifies the CPU benchmarks into low, medium and high memory intensity based on the observed MPKI values as seen at the L2 cache when running alone. We create a multi-programmed workload with 4 SPEC benchmarks each. We use the Rodinia benchmark suite \cite{rodinia} to represent GPU applications that offload kernel computation to a GPU CUs. These Rodinia applications are modified to elide the \textit{memcpy} api calls so as to run with unified virtual and physical address spaces (Rodinia-nocopy). 

\begin{table}[htb]
	\centering
	\begin{tabular}{|l|l|}
		\hline
		Low (MPKI $\leq$ 10)          \hspace{5em}      & astar, cactusADM, gcc, gobmk, hmmer, sphinx3    \\ \hline
		Medium (10 \textless MPKI $\leq$ 20) & bwaves, bzip2, leslie3d, libquantum, milc \\ \hline
		High (MPKI \textgreater 20)            & mcf, soplex       \\ \hline
	\end{tabular}
	\caption{MPKI of CPU Stand alone Benchmarks}
	\label{single-cpu-mpki}
\end{table}

\begin{table}[htb]
	\centering
	\begin{tabular}{|l|l|}
		\hline
		Low (MPKI $\leq$ 10)                & hotspot, lud           \\ \hline
		Medium (10 \textless MPKI $\leq$ 20) &  kmeans, needle, srad   \\ \hline
		High (MPKI \textgreater 20)            & streamcluster, gaussian       \\ \hline
	\end{tabular}
	\caption{MPKI of GPU Stand alone Benchmarks}
	\label{single-gpu-mpki}
\end{table}

For the Rodinia-nocopy benchmarks we used \textit{needle}, \textit{k-means}, \textit{gaussian}, \textit{hotspot}, \textit{srad}, \textit{streamcluster} and \textit{lud}. The rest of the programs either could not be compiled for IHS architecture or could not run (results in forward progress deadlock errors in the modified simulator). The Rodinia benchmarks use the fifth CPU core to run the initialization section before offloading the kernel to GPU. Thus, our IHS simulates five CPU cores and eight GPU CUs. Table \ref{single-gpu-mpki} categorizes the Rodinia-nocopy benchmarks into Low, Medium and High based on the observed MPKI values (Instructions here refer to thread instructions) at the L2 cache.


\par Combinations of these SPEC quad-core multi-programmed workloads were coupled with a full Rodinia-nocopy benchmark to create a representative mix of applications \ref{workloads} that would run in an IHS system. These composite workloads embody different levels of total memory intensity produced by the CPU and GPU cores.
\par We also measure the footprint of these workloads in terms of number of unique 128B cache blocks accessed at the DRAM cache. The memory footprints range from 70MB to 650MB for quad-core CPU workloads and from 5.5MB to 135MB for the GPU application. The smaller footprints obligates us to use a smaller stacked DRAM cache capacity to make pertinent observations.

\begin{table}[h]
  \centering
  \begin{tabular}{{|l|l|l|}}
    \hline
    \textbf{Name} & \textbf{Multi-program SPEC2006} & \textbf{Rodinia}\\
    \hline
    Qg1& cactusADM;gcc;bzip2;sphinx3 & needle\\
    \hline
    Qg2 & astar;mcf;gcc;bzip2 & needle\\
    \hline
    Qg3 & gcc;libquantum;leslie3d;bwaves & needle\\
    \hline
    Qg4 & astar;soplex;cactusADM;libquantum & k-means\\
    \hline
    Qg5 & milc;mcf;libquantum;bzip2 & k-means\\
    \hline
    Qg6 & bzip2;gobmk;hmmer;sphinx3 & k-means\\
    \hline
    Qg7 & soplex;milc;cactusADM;libquantum & gaussian\\
    \hline
    Qg8 & milc;libquantum;gobmk;leslie3d & gaussian\\
    \hline
    Qg9 & astar;milc;gcc;leslie3d & hotspot\\
    \hline
    Qg10 & gcc;gobmk;leslie3d;sphinx3 & hotspot\\
    \hline
    Qg11 & astar;cactusADM;libquantum;sphinx3 & srad\\
    \hline
    Qg12 & astar;mcf;gobmk;sphinx3 & streamcluster\\
    \hline
    Qg13 & astar;cactusADM;libquantum;sphinx3 & lud\\
    \hline
  \end{tabular}
  \caption{Composite Heterogeneous Workloads}
  \label{workloads}
\end{table}

\section{Simulation Methodology} \label{simulation-methodology}
We fast-forward the initialization phase of each workloads up until just before the launch of the first kernel of the GPU program. We ensure that each executes at least 2 Billion instructions in fast forward and a total of 20 billion instructions of the IHS workloads is executed on an average in the CPU cores. This is accomplished by adding a pause phase to the Rodinia benchmarks for the duration until the initialization quota of the SPEC programs is complete. The benchmarks were then check-pointed after the above fast forward phase. Subsequent detailed simulations with different configurations were run from this checkpoint.
We then warm the cache until the fastest core completes 250 million instructions. During the warmup phase the GPU program is not executed. Timing simulations were then run for at least 250 million instructions for each CPU core, resulting in a total of more than 1 Billion instructions across all the CPU cores and the kernel is executed on he GPU core using the gpgpu-sim component. As is the norm, when a core finishes its quota of 250 million instructions, it continues to execute until all the cores have completed.
\par As stated earlier, the Rodinia application uses the 5\textsuperscript{th} CPU core and offloads the kernel to the integrated GPU. These applications are modified to execute in a \textit{conditioned loop} such that there is no corruption of data structures in the program. The \textit{conditioned loop} is run infinitely and represents the region of interest (ROI) of the Rodinia benchmark. This region includes sections of CPU activity and GPU offloads in the execution, as is typical of a HSA program which will exhibit an interleave of offloaded region and serial CPU section. However, only the performance statistics for the first execution of the \textit{conditioned loop} in the program are considered. 
This is done as the first loop represents the true run of the GPU kernel. The number of \textit{conditioned loops} executed depends on the length of the simulation and the IPC achieved by the GPU. Also, Subsequent loops can achieve better hit rates in cache due to the data fetched by the earlier loops thus not corroborating with the true performance achieved.
In cases where the ROI is longer than the complete run of the CPU workloads, the statistics for the last completed kernel are used. 

\section{Performance Metrics} \label{perf-metrics}
Our study concerns the performance of the IHS architecture, in which multiple single threaded CPU programs and GPU program are co-run and contend for the shared DRAM cache. 
In such a co-run setup, the progress made by the applications (workloads) is different compared to when running homogeneously (i.e., only ran CPU applications on CPU cores or GPU kernels on GPU cores). Further, the performance impact of the shared DRAM cache is not same on each of the CPU benchmarks as well as the GPU benchmark. We require to define metrics that combine IPC meaningfully without biasing the system performance to either CPU or GPU cores. Hence, we report a multitude of performance metrics to holistically determine the performance of the IHS processor with the DRAM cache and our optimizations. 
\par We report performance of each processor using the intuitive metric of harmonic mean of IPC for the CPU cores and GPU CUs which is defined as 
{
\begin{equation*}
H\textnormal{-}MEAN_{CPU} = \frac{n_{cpu}}{\sum_{i=1}^{n_{cpu}} \frac{1}{IPC_{i}^{CPU}}} \ and \hspace{0.2cm} H\textnormal{-}MEAN_{GPU} = \frac{n_{gpu}}{\sum_{i=1}^{n_{gpu}} \frac{1}{IPC_{i}^{GPU}}} 
\end{equation*}
}
where $IPC_i^{CPU}$ and $IPC_i^{GPU}$ are the instructions per cycle achieved by the $i^{th}$ CPU core and $i^{th}$ GPU CU respectively and $n_{cpu}$ and $n_{gpu}$ are number respectively the number of CPU cores and GPU cores.
In this work, we report CPU and GPU $H\textnormal{-}MEAN$ (and the improvement in them) independently. This is done to identify the impact of CPU and GPU applications on each other, as well as to understand the impact of the proposed modifications on each of the application type. 

\par To report combined system performance we use 2 metrics; the H-Mean of all IPCs and Weighted Speedup for all IPCs. This is important as we would need to see a fair system performance metric without biasing the metric to either CPU or GPU performance. Thus, we report combined system performance using the combined H-MEAN of IPC all CPU and GPU cores which is defined as,
{
\begin{equation*}
H\textnormal{-}MEAN_{IHS} = \frac{n_{cpu}+ n_{gpu}}{\sum_{i=1}^{n_{cpu}} \frac{1}{IPC_{i}^{CPU}} + \sum_{i=1}^{n_{gpu}} \frac{1}{IPC_{i}^{GPU}}} 
\end{equation*}
}
\par Combined system performance using the weighted speedup metric \cite{weighted-speedup} is defined as,
{
\begin{equation*}
Weighted Speedup = \sum_{i=1}^{n_{cpu}} \frac{IPC_i^{CPU_{IHS}}}{IPC_i^{SP}} + \sum_{i=1}^{n_{gpu}} \frac{IPC_i^{GPU_{IHS}}}{IPC_i^{GPU}}
\end{equation*}
}
where ${IPC_i^{CPU_{IHS}}}$ and $IPC_i^{GPU_{IHS}}$ denote the IPC achieved by the $i^{th}$ CPU core and the $i^{th}$ GPU CU when running in a IHS setup respectively. $IPC_i^{SP}$ denotes the IPC of the $i^{th}$ CPU application running in a single core in a stand alone mode. $IPC_i^{GPU}$ denote the IPC of the $i^{th}$ GPU CU when the GPU application is running in a stand-alone mode across all GPU CUs. 

\section{Summary}
In this chapter, we presented the simulator extensions to the gem5-gpu simulator to model a DRAM cache for an IHS architecture. We described the workloads created to evaluate the proposed design and the simulation methodology. Finally, we define the comprehensive performance metrics used to measure the IHS system performance for the various configurations.

