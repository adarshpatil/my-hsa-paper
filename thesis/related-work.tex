\chapter{Related Work} \label{chap:related-work}
\section{3D Stacked DRAM devices}
Stacked DRAM devices have primarily been organized to improve the performance of multi-core architectures. Principally there have been 2 schools of thought for using the stacked DRAM devices 
\begin{itemize}
	\item as an addressable part-of-memory stacked DRAM 
	\item as a hardware managed stacked DRAM Cache
\end{itemize}
\subsection{Part-of-memory}
In \cite{pom,cameo} the stacked DRAMs are organized as part of memory to in lieu of the large capacities provided by these devices. The designs propose hardware management schemes for swapping hot pages into and out of the stacked DRAM devices. These designs can potentially suffer from large swapping overheads due to the large and disparate working sets of IHS workloads increasing the number of hot pages in the system. Orthogonal to these, in \cite{software-dram} the authors propose to expose the stacked DRAM to the applications and/or system software by providing special allocation calls or using intelligent page management algorithms in system software to place hot data in the high bandwidth memory. Besides these designs incurring the obvious overheads of software modification to improve performance, they also require a good understanding of program behaviour and IHS architecture knowledge.
\subsection{Hardware Managed Cache}
There are several works that propose various organizations for using stacked DRAMs as hardware managed cache. All of these works were done with homogeneous multi-core CPUs or GPUs. Our work builds on this body of literature and looks at cache management issues for heterogeneous architectures. We look at some of the significant classes of work below.
\begin{itemize}
\item \textbf{Cache Granularity}: In the works of \cite{loh-hill,alloy,atcache,bimodal} they propose caching at smaller granularities ($<$512B) and store the metadata in the rows (pages) of the stacked DRAM. These works also propose various types of predictors like \textit{miss-map}, \textit{MAP-I}, caching a subset of tags, way predictor etc. to determine if a line is present in the DRAMCache. \cachename\ adapts the Alloy Cache organization and the \textit{MAP-I} predictor to IHS architectures. In the works of \cite{footprint}, the authors propose allocation at larger page granularity (e.g., 2KB, 4KB) but only fetching useful footprint of the page into the stacked DRAMCache. In Section \ref{design}, we have justified the design decision of caching granularity in \cachename.

\item \textbf{Access Partitioning:} The works of \cite{mostly-clean,mainak-hpca,micro-refresh,bear} look at access paritioning between DRAMCache and off-chip DRAM as the latency of access to these devices is mostly similar. Paritioning the accesses between both these DRAM class devices can improve effective bandwidth of the system. \cachename\ uses the ingrained disparity in the requests rates and their implication on performance of each core to optimize bandwidth balance in a heterogeneity aware manner. We also compare the performance of \cachename\ with MCC \cite{mostly-clean} in section \ref{comparison}.

\item \textbf{TLB assisted DRAMCaches:} These DRAMCache organizations piggyback on the TLB translation mechanisms to infer the presence of blocks in the cache. Tagtables \cite{tag-tables} achieves this by flipping the page table organization, which requires modification to the TLBs and page walker that are hardware managed in x86 architectures. Tagless DRAMCache \cite{tagless-dramcache} provides a more solution which translates virtual address to a die-stacked cache address and moves the translation to physical address off critical path. The GPUs in IHS architectures have several threads running concurrently which already stresses the address translation structures like TLB and page walkers. Overloading these structures to further add DRAMCache meta-data would require careful design decisions and we defer this to future work.

\end{itemize}
\section{IHS architectures}
Complementary to our work, these body of works discussed below propose optimizations to improve the performance of IHS architectures. 
\par There have been efforts to improve performance of IHS systems in \cite{gpu-concurrency} by throttling the GPU cores using intelligent warp scheduling. The work in \cite{interconnect} deals with designing an effective network-on-chip interconnect for IHS architectures. 
\par \textbf{On-chip shared cache management:} To manage shared on-chip SRAM caches, Lee et al. \cite{tap}  propose heterogeneity aware schemes that are built on top of UCP and RRIP schemes for managing shared resources. While our chaining mechanism is in line to this to ensure minimum occupancy for CPU requests, it also goes beyond by introducing pseudo-associativity and improving hit-rates (specifically for GPU requests). Mekkat et al. \cite{helm} further propose shared SRAM cache management for IHS workloads that uses runtime metrics, like cache sensitivity of each workload, to allocate cache capacities. Despite larger capacities, DRAMCaches have higher latencies and hence will not be able to adapt quickly to SRAM occupancy management schemes proposed in these works. 
\par Zhan et al. \cite{oscar} propose improving performance of IHS architectures by replacing on-chip SRAM caches with slightly larger STT-SRAM caches that are non-volatile but have asymmetric read/write energy and latencies. They focus on NoC related optimizations through NoC reordering/batching schemes and differential CPU/GPU, read/write prioritization. The NoC optimizations are orthogonal and can be supplemented to the ideas proposed in this work. The performance improvement due to introduction of STT-RAM in IHS architectures is equivalent to that observed in our naive DRAMCache.
\par \textbf{Memory Access Scheduling:} Lastly, Ausavarungnirun et al., \cite{sms} propose staged memory scheduling for main memory (DRAM) in IHS processors and is similar in spirit to our \prioname, which is applied at the DRAMCache. \prioname\ uses a relatively simpler approach for prioritizing CPU requests while ensuring that GPU request are not significantly starved. We also compare the performance of \cachename\ against SMS in Section \ref{comparison}. The authors in \cite{qos-aware} propose a QoS aware memory scheduler to avoid the GPU from missing a frame rendering deadline. However, in our IHS architecture the GPU is used to accelerate general purpose code and hence PrIS does not consider such deadlines for the memory controller.
\par \textbf{Heterogeneous Coherence:} As noted earlier, IHS architectures provide a coherent memory between CPU and GPU. The coherence protocol accordingly has to be altered to avoid GPUs overwhelming the caches with coherence requests due to their large multi-threaded execution model. In \cite{hsc-coherence}, Power et. al. tailor the coherence protocol for IHS architectures. It uses a region-based coherence and reduces large resource requirements.

\section{Benchmarks}
To exploit the full potential of IHS architecture a workload needs to be able to use both the types of cores suitably in its working. Recently Juan et al. authored the Chai benchmarks suite \cite{chai} that consists of workloads that collaboratively use both and CPU and GPU for execution. The Chai benchmarks have data partitioned, fine and course grain task partitioned execution paradigm benchmarks. For the purposes of our study the shared DRAMCache is receiving requests simultaneously from CPU and GPU which already simulates a busy IHS architecture. These benchmarks could provide better insights into working of IHS programs but broadly we expect the inferences of \cachename\ to be similar with these benchmarks as well.

\section{Summary}
In this chapter, we described the related work in the area of memory systems, IHS architecture and collaborative workloads for these architecture. To the best of our knowledge, no existing approach are aimed at die stacked DRAM organization for IHS architecture. We discuss how \cachename\ mechanisms differ from state-of-the-art techniques. We show that even though the area of DRAMs and DRAMCache are explored by several works, incorporating the effects of heterogeneity in IHS architectures is an important consideration while designing DRAMCaches.