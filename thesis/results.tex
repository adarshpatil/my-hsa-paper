\chapter{Performance Evaluation of \cachename} \label{chap:results}
In this chapter, we evaluate the performance of the proposed \cachename\ mechanisms discussed in Chapter \ref{chap:hashcache} and show that \cachename\ is able to improve the performance of IHS. Further, we show that \cachename\ solution is robust and scales well by performing sensitivity study for the various components in Section \ref{sensitivity-results}. Lastly, we compare our schemes quantitatively with a couple of state-of-the-art techniques in Section \ref{comparison}

\section{\cachename\ Mechanisms}
In this section, we present the performance of \cachename\ mechanisms (\prioname, \bypassname\ and \chaining) and some combinations of them. We examine the impact of these mechanisms on CPUs, GPUs and the performance of the IHS system as a whole. Throughout, we use the experimental methodology as described in Section \ref{simulation-methodology}. 
\par In the following subsections we discuss the results in detail for each of the \cachename\ schemes in detail. Figure \ref{results-speedup} (a) and (b) presents the performance improvement in terms of Harmonic Mean of IPC, for CPU and GPU respectively, normalized to the baseline without a DRAM cache. We report the performance improvement due to the introduction of DRAM cache (Naive),  and then the different HASHCache mechanisms  and their combinations. In all cases the CPU applications are co-run with the GPU applications. For comparison reasons, we also show the performance of HoA CPU --- CPU applications (multi-programmed workloads) when run without the GPU application --- and  HoA GPU --- GPU application run without CPU applications.

\begin{figure}[htb]
	\centering
	\includegraphics{graphs/results-cpu}
	\includegraphics{graphs/results-gpu}
	\caption{Speedups obtained by adding a stacked DRAM cache for (a) CPU (b) GPU}
	\label{results-speedup}
\end{figure}

\subsection{PrIS DRAM cache Scheduling}
Prioritizing CPU requests with our \prioname\ scheduler at the DRAM cache controller leads to considerable performance benefits for the CPU applications. By using \prioname, the average access latency of the DRAM cache for CPU requests reduces by an average of 15.3\% and upto 48.9\% over a naive DRAM cache (see Figure \ref{results-pris}). Hence, \prioname\ is able to improve the performance of the CPU by an average of 35\% over a naive DRAM cache. However, on the flip side giving aggressive priority to all CPU requests reduces the performance of GPU by 10\% despite the GPU being able to tolerate larger memory access latencies. For some of the benchmarks like Qg3, Qg10, Qg11 and Qg12 the high priority given to CPU requests by \prioname\ impacts the GPU, causing the GPU performance to reduce marginally below the baseline IHS architecture without a DRAM cache. Note however that, in these workloads, the introduction of DRAM cache (naive) itself improves performance only marginally. Our mechanisms (\bypassname and \chaining) further aim to reduce this performance drop for the GPU by ensuring (a) there are fewer CPU requests in the DRAM cache queues (b) GPU requests, despite being de-prioritized at DRAM cache, have a better hit rate in the DRAM cache and avoid accessing the off-chip DRAM.

\begin{figure}[htb]
	\centering
	\includegraphics{graphs/results-pris}
	\caption{Reduction in DRAM cache Access Latency for CPU requests with \prioname\ over a naive DRAM cache}
	\label{results-pris}
\end{figure}

\subsection{ByE for Temporal Bypass}
\bypassname\ attempts to improve performance by directing some CPU requests to be served from the off-chip DRAM, thus achieving improved resource utilization and bandwidth balance in the process. This also ensures that the overall aggregate system bandwidth can be exploited efficiently. \bypassname\ alone achieves 12\% improvement in CPU performance and a 3\% improvement in GPU performance over a naive DRAM cache.
\par The CPU performance improvements are primarily due to bypassed requests facing reduced queuing delays at DRAM. Figure \ref{results-bloom} shows the percent reduction in total memory access latency on primary y-axis (left) for CPU read requests achieved by \bypassname\ over an already aggressive naive DRAM cache which employs a hit/miss predictor for CPU requests. The total memory access latency for CPU read requests reduces by an average of 28\%. The already high hit rates for GPU in the DRAM cache coupled with the no-fill policy for bypassed CPU requests ensures fewer GPU requests at the off-chip DRAM which leads to less congestion. 
\par Figure \ref{results-bloom} also shows the percentage of incoming read requests bypassed by \bypassname, on the secondary y axis. 
\bypassname\ is able to bypass on an average about 37\% of incoming read requests. On an average 23\% read requests are to dirty lines in the cache which cannot be bypassed.  The remaining 40\% are false positives in our Bloom filter implementation which could have bypassed the DRAM cache. We discuss more on a sensitive study of Bloom filter later in this section. Further, the reduced set contention and less number of CPU requests in DRAM cache queues reduces congestion for GPU, which in turn leads to small performance benefits for the GPU as well improving the Harmonic Mean of GPU IPC by 3\% over naive DRAM cache.

\begin{figure}[!htb]
    \centering
    \includegraphics[scale=1.05]{graphs/results-bypass}
    \caption{Total MemAccLat reduction with \bypassname\ and \% of bypassed read requests for CPU requests}
    \label{results-bloom}
\end{figure}

\par Combining \prioname\ with \bypassname\ allows for the non-bypassed CPU requests at the DRAM cache to be served with a higher priority and hence reducing the queuing delays. \bypassname+\prioname\ performs better than just \prioname\ by 10\% for CPU and 7\% for GPU. Overall \bypassname+\prioname\ achieves 48.5\% improvement in CPU performance over a naive DRAM cache while degrading GPU performance by just 3\%.


\par Another result to be noted here is that, unlike \prioname\ or \prioname+\bypassname, \bypassname\ is able to improve not just CPU application performance but also achieves improved performance for GPU applications over a naive DRAM cache. \bypassname\ achieves this by being able to reduce the interference of CPU and GPU requests at DRAM cache, thus allowing the GPU to better utilize the DRAM cache bandwidth and capacity while the redirected CPU requests are served out of off-chip DRAM memory.

\begin{figure}[htb]
	\centering
	\includegraphics{graphs/results-large-bloom}
	\caption{Performance with a larger Bloom Filter}
	\label{large-bloom}
\end{figure}

\par The somewhat high false positive ratio in our bypass mechanism is due to the large number of dirty blocks in the DRAM cache and the relatively small size of the Bloom filter.  We also experimented with 3 hash-functions while also optimally increasing the array capacity to 312KB (20\% larger) to reduce aliasing and increase the efficacy of bypass. The performance of CPU and GPU using the larger Bloom filter (in terms of Harmonic Mean of IPC) normalized to the original bloom filter size is shown in Figure \ref{large-bloom}. We observe that the CPU performance improves only by 2.1\% on an average while the GPU remains largely unaffected. Hence, we retain the original Bloom filter (256KB) with 2 hash functions for our \bypassname\ implementation.

\subsection{Chaining for Spatial Occupancy Control}
As discussed in Section \ref{mechanism-chaining}, \chaining\ mechanism improves hit rates for GPU while guaranteeing some occupancy for the CPU in the DRAM cache. We empirically determine a suitable low occupancy threshold of the CPU (\textit{$l_{cpu}$}) to be 20\% for all our workloads. Chaining alone performs no better than a naive cache as the queuing latencies overwhelm any improvements in hit rates. However, when chaining is coupled with \prioname, the increased hit rates reduces the performance drop caused by \prioname\ for the GPU from 10\% to 5.2\% (i.e., 4.8\% performance improvement over \prioname). For the CPU, guaranteed occupancy in the DRAM cache and the secondary effect of lower congestion at off-chip DRAM allows CPU requests to be serviced with lower delays. This improves performance of CPU by 7\%, over only \prioname.
\par Overall \chaining+\prioname\ improves CPU performance by 44.7\% while degrading GPU performance by merely 4.8\% over a naive DRAM cache.
% %This improves overall system throughput by a significant 37.7\% over a un-optimized DRAM cache.

\subsection{Summary of \cachename\ Mechanisms Performance}
We now holistically examine the performance improvements due to the introduction of our \cachename\ mechanisms, in  CPU and GPU cores together. From Figure \ref{results-speedup} we observe that, adding a naive DRAM cache can achieve an average of 42\% and 24\% improvement in CPU and GPU cores respectively. Whereas, \cachename\ achieves significant speedups of (105\%,17.5\%) and (111\%,20.4\%) for (CPU,GPU) using Heterogeneity-aware mechanisms of \bypassname\ and \chaining\ respectively. This comes within 16\% and 13\% of the ideal no interference performance for the GPU (final gray bar in Figure \ref{results-speedup}) and within 81\% and 76\% of the ideal no interference performance for the CPU (final gray bar in Figure \ref{results-speedup}(b)) for each of the schemes. Further, for memory intensive combination of CPU and GPU workloads like Qg7 and Qg8 which see significant degradation in performance of both processors due to interference, adding a DRAM cache can improve performance upto 330\% for CPU and 48\% for GPU over a baseline system with no stacked DRAM cache. In comparison, the naive DRAM cache implementation only brings 55\% and 56\% improvement in the CPU and GPU performance respectively.

\subsection{System Performance with \cachename}
\begin{figure}[!htb]
	\centering
	\includegraphics{graphs/results-system}
	\includegraphics{graphs/results-weighted-speedup}
	\caption{(a)Harmonic Mean (b)Weighted Speedup of IHS with \cachename}
	\label{results-system}
\end{figure}
\par We now examine aggregate IHS performance using system metrics - H-Mean of IHS IPCs and Weighted Speedup of IHS IPCs, as discussed in Section \ref{perf-metrics}. First, Figure \ref{results-system}(a) plots the performance improvement as a Harmonic mean of IPCs of IHS IPC (combined CPU cores and GPU CUs), normalized to our baseline IHS architecture without a DRAM cache. Overall with simple heterogeneity aware management of the stacked DRAM cache, IHS systems can achieve, on an average, 103\% (upto 270\%) performance improvement while a naive DRAM cache is able to achieve just 41.8\% improvement.
\par Second, Figure \ref{results-system}(b) plots the weighted speedup normalized to the baseline of IHS without DRAM cache. Adding a DRAM cache to IHS processors naively achieves an improvement of merely 3\%. In fact for some workloads like Qg3 and Qg11 adding a DRAM cache without careful considerations can lead to negative performance impact. However, our heterogeneity-aware mechanisms are able to achieve on an average of 16\% and 15\%  improvement in performance over a IHS architecture without a DRAM cache. This improvement corresponds to a 12.9\% and 11.5\% improvement for each of the schemes over a carefully designed but heterogeneity unaware DRAM cache for the IHS processors.


\section{Sensitivity Study} \label{sensitivity-results}

Next we study the sensitivity of \cachename\ mechanisms with regard to a few memory system parameters.

\subsection{Larger Capacity DRAM cache}


\begin{figure}[!htb]
	\centering
	\includegraphics{graphs/results-128M-cpu}
	\includegraphics{graphs/results-128M-gpu}
	\caption{Sensitivity Study with 128MB DRAM cache (a)CPU (b)GPU}
	\label{results-128m}
\end{figure}

We conduct a sensitivity study for \cachename\ mechanisms with a larger 128MB stacked DRAM cache. Figure \ref{results-128m} presents the performance of a naive DRAM cache and our mechanisms \bypassname+\prioname\ and \chaining+\prioname\ for the (a)CPU and (b)GPU in terms of H-Mean of IPC normalized to an IHS with no DRAM cache. We observe that the larger naive DRAM cache is able to achieve 54.5\% and 29.8\% improvement for CPU and GPU processors. \cachename\ mechanisms continue to provide an average improvement of 46\% and 39\% for the CPU, on an average, above a heterogeneity-unaware DRAM cache and 126\% and 115\% over the baseline IHS. 
In the larger DRAM cache, Chaining+PrIS results in 15.8\% improvement over baseline (10\% decrease over naive), we
expect the scheme to catch up the lost performance (over naive) in benchmarks with larger GPU footprint.


\subsection{Off-chip DRAM with same latency as DRAM cache}

\begin{figure}[!htb]
	\centering
	\includegraphics{graphs/results-ddr3-2133-cpu}
	\includegraphics{graphs/results-ddr3-2133-gpu}
	\caption{Sensitivity Study with DDR3-2133 off-chip DRAM (a)CPU (b)GPU}
	\label{results-2xbw}
\end{figure}

Although we expect the smaller sizes of stacked DRAMs to have lower latency than off-chip DRAMs, commercial stacked DRAM like those in Intel's Knights Landing \cite{xeonphi} have access latencies similar to off-chip DRAMs. Hence, we scale up the off-chip DRAM to DDR3-2133 like device with latencies (13.09ns-13.09ns-13.09ns-33ns) for $t_{CL}$-$t_{RCD}$-$t_{RP}$-$t_{RAS}$. As before, Figure \ref{results-2xbw} presents the performance (H-Mean of IPC) of (a)CPU (b)GPU with a naive DRAM cache and \cachename\ normalized to a IHS system with no DRAM cache. For \prioname+\bypassname, CPU performance improves by 48\% while reducing merely 3\% of GPU performance. For \prioname+\chaining, CPU performs 47\% better than a naive DRAM cache while decreasing GPU performance by 4.4\%. Thus, \cachename\ mechanisms continue to benefit even when stacked DRAM caches have comparable latency to off-chip DRAM memory.

\subsection{Higher Bandwidth DRAM cache}

\begin{figure}[!htb]
	\centering
	\includegraphics{graphs/results-2xbw-cpu}
	\includegraphics{graphs/results-2xbw-gpu}
	\caption{Sensitivity Study with 2x Bandwidth for DRAM cache (a)CPU (b)GPU}
	\label{results-ddr3-2133}
\end{figure}

We carry out experiments for \cachename\ with a higher bandwidth for stacked DRAM cache (80GB/s). We achieve this by doubling the burst length of the stacked DRAM cache devices. Again, Figure \ref{results-ddr3-2133}  presents the performance of a naive DRAM cache and \cachename\ for (a)CPU and (b)GPU. We observe that our mechanisms scale with increased DRAM cache bandwidth and \bypassname+\prioname\ performs 51\% better and \chaining+\prioname\ performs 47.3\% better than a naive DRAM cache for CPU. GPU performs also improves by 1.9\% and 2.4\% over a naive DRAM cache. Notably for GPU benchmarks like \textit{streamcluster} and \textit{gaussian}, \chaining+\prioname\ performs better than \bypassname+\prioname.

\section{Comparison with Related Work} \label{comparison}
The objectives of \bypassname\ and \prioname\ are, respectively, similar to the Mostly Clean DRAM cache (MCC) \cite{mostly-clean} and Staged Memory Scheduling (SMS) \cite{sms} respectively. 
\par SMS \cite{sms} is a DRAM memory access scheduler for IHS architectures. SMS decouples the various tasks for memory access scheduling into three stages. The first stage groups requests based on row-buffer locality from the same requester core. The second stage focuses on policies for inter-application request scheduling i.e., picking one of the request queues from first stage. The third stage consists of FIFO queues that issue DRAM commands while respecting DRAM timing constraints. We use a total request queue size of 64 entries for the first stage in SMS. For each of the four CPU cores we use a 10 entry first stage request queue and a single 24 entry first stage request queue for all the GPU cores which is similar to the specifications in the original SMS work \cite{sms}. We note that the total request queue size of 64 entries is the same for \prioname\ as well. Additionally, for the second and third stage of SMS we use a simple 10 entry FIFO queue each.
\par MCC \cite{mostly-clean} is a DRAM cache scheme proposed for multi-core CPUs, that attempts to improve performance by dispatching some of the requests to off-chip DRAM. For this they use a hybrid write policy in the DRAM cache that supports write-through and write-back policies for different rows (pages) of the DRAM cache. By permitting only a limited number of pages to be in write-back mode, MCC is able to bound the amount of dirty data in the DRAM cache. This enables the \textit{Self-Balancing Dispatch (SBD)} mechanism to redirect some of these requests to the off-chip DRAM to effectively use the under-utilized off-chip DRAM bandwidth. For MCC, we implement the \textit{Dirty Region Tracker (DiRT)} structure as proposed in the original work \cite{mostly-clean}, which tracks the number of writes to different pages using a counting Bloom filter and a Dirty List that holds the list of pages that are operated in write-back policy mode. The DiRT structure lookup determines if the request can be sent off-chip. If the lookup into DiRT returns that the access was to a clean row (page), the request is sent to the off-chip DRAM, else it is enqueued in the DRAM cache queue.
\par A more comprehensive description of the working and mechanisms of SMS and MCC can be found in Section \ref{chap:related-work}. Here we quantitatively compare \cachename\ with MCC (adapted for IHS architecture) and SMS. Figure \ref{results-comparison} presents performance of CPU and GPU normalized to a IHS with no DRAM cache.

\begin{figure}[!htb]
	\centering
	\includegraphics{graphs/results-comparison-cpu}
	\includegraphics{graphs/results-comparison-gpu}
	\caption{Comparison of \cachename\ against MCC \cite{mostly-clean} and SMS \cite{sms} (a)CPU (b)GPU}
	\label{results-comparison}
\end{figure}
 
\par Overall, \bypassname\ performs 7.8\% better than MCC in terms of CPU IPC. This is because when a page (row) is marked as write-through in MCC, all requests to that page are bypassed, oblivious of source of the request. The large number of GPU requests to the cache lines in the write-through (clean) pages quickly exhausts the available MSHR entries which in turn leads to the DRAM cache being blocked for subsequent requests. For workloads Qg9, Qg10 and Qg12 the GPU workloads are relatively less intensive and MCC tends to perform slightly better than \bypassname. The GPU performance is comparable for both approaches.
\par The striped bars in Figure \ref{results-comparison} show the performance of the \prioname\ and SMS for CPU and GPU. For SMS implementation, we proportionally scale down the total hardware requirements to the same size as that of \prioname\ (128 entries). We observe that \prioname\ performs on-par with SMS for almost all CPU workloads. For workloads like Qg2 and Qg12 SMS performs marginally better than \prioname\ as it is able to better manage CPU inter-application interference using Shortest Job First (SJF) scheduling. This SJF scheduling reduces latencies for CPU applications as they are serviced quicker, hence reducing the waiting time. However for the GPU, SMS performs 4\% better than \prioname\ due to its batching algorithm that is able to admit row buffer hit requests into the queues which leads to improved row buffer hit rate (9\% better). 
\par Finally, we note that our combined mechanism of \prioname+\bypassname\ proposed in this work performs better than each of prior compared schemes for both CPU and GPU.

\section{Summary}
In this chapter, we evaluated the performance of \cachename\, its constituent mechanisms and some of their combinations, first on individual CPU and GPU cores and subsequently on IHS architecture as a whole. Next we perform sensitivity studies for capacity, bandwidth and latencies of the memory subsystem, thus showing that \cachename\ is robust and performs well for wide variety of memory system parameters. Finally we also present quantitative comparison of \cachename\ with two of the state-of-the-art mechanisms --- Staged Memory Scheduling \cite{sms} and Mostly Clean DRAM cache \cite{mostly-clean}.