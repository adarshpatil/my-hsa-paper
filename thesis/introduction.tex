\chapter{Introduction}
\label{chap:introduction}

%\begin{quote} \small



\par The remarkable advances in computing power of the modern microprocessor over the last few decades can predominantly be attributed to shrinking of feature sizes. Miniaturization of transistors has allowed addition of specialized on-chip hardware circuitry for acceleration units. 
A study in 2010 by Koomey et. al \cite{koomey} found that the amount of computation that could be done per unit of energy doubled about every 18 months. However, to reach exascale and beyond requires a thousand fold decrease in energy consumed per flop computed.
Graphics processing units (GPUs) have evolved from being fixed-function pipelines and are being used to accelerate data parallel code for general purpose (GP) applications. Compared with multi-core CPUs, GPGPUs offer the potential for better performance of data parallel code at lower energy. 
There has been considerable effort in recent years to develop GPU implementations of a wide variety of parallel algorithms from the High Performance Computing (HPC) and the enterprise domains. GPGPUs architectures and compiler transformations have evolved over several iterations and are now able to efficiently execute programs with irregular branches and memory behaviour as well. 
Traditionally these discrete processors have had their own independent memory systems. To take advantage of the discrete GPUs, the CPU must copy data to the GPUs memory and back. This data movement is wasteful and expends energy while also adding latency as the transfer happens over a slower PCIe bus. The separate address spaces and complex programming models that need to manage two sets of data further impede expansion of the workloads that benefit from GPGPU computing. 

\par In contrast to the processing capabilities of the modern microprocessor, DRAM memory speeds have not kept pace commensurately to serve the increasing demands of the processors. This speed imbalance coupled with a limited growth in pin counts has led to the memory and bandwidth wall \cite{memory-wall,bandwidth-wall} for off-chip DRAM systems which often becomes a performance limiting factor. The key challenge in large multi-core processors is the requirement of memory bandwidth to keep the cores running without stalls. For this, each generation usually increases memory channels with each DDR generation, which increases memory system power consumption.
A typical DRAM device organization contains banks where each bank contains a 2D array of storage cells, row and column decoders, sense amplifiers and peripheral circuits. Banks are further organized into ranks which share command busses but have separate data busses. A chip select is used to select a bank to issue a command. Commercially DRAMs are available as DIMM (Dual in-line memory module) containing several DRAM devices with a standard connector. A memory controller controls the operation of the DRAM device while respecting timing constrains and bus conflicts.


%\end{quote}

\section{Intergrated Heterogenous Systems (IHS)}
\begin{figure}[!htb]
	\centering
	\def\svgwidth{0.7\columnwidth}
	\input{figures/arch.pdf_tex}
	\caption{Architecture of an Integrated Heterogeneous System}
	\label{fig:hsa-arch}
\end{figure}

\par Modern processor chips have heterogeneous processors which integrate a lower capacity GPU on-die allowing for graphics rendering only. In view of the widespread use of the GPU for general purpose applications, processor manufacturers including AMD\cite{amd-apu}, Intel\cite{inteliris}, and NVIDIA\cite{denver} are beginning to allow general purpose OpenCL\cite{opencl}/CUDA\cite{cuda} programs to run on their Integrated Heterogeneous System (IHS) platform which were so far restricted only to graphics rendering. The HSA Foundation \cite{hsafoundation} was setup to develop and define cross-vendor hardware specifications and software development tools needed to allow application software to better use this IHS architecture. 

\par The IHS architecture, as illustrated in Figure \ref{fig:hsa-arch}, consists of multiple CPU cores with private L1 and shared L2 caches along with multiple GPU cores or Compute Units (CUs) with private L1 and shared L2 cache. These caches (except GPU L1 caches) are kept coherent with a heterogeneous MOESI-like coherence protocol. The Network-on-chip (NoC) connects the cores with the caches and the memory controllers, while the memory itself is off-chip and connected via memory channels. 
\par The IHS architecture supports a shared virtual address space making pointer sharing semantics possible between CPU and GPU which simplifies programming. Further, a shared physical address space and the cache coherent interconnect reduces GPU initialization time and enables several high level languages \cite{sumatra,julia} to also take advantage of the parallel processing synergistically with the CPU. Programmers can now write applications that seamlessly integrate CPUs with GPUs while benefiting from best attributes of each. Fine-grain parallel stream processing like face detection, compression, encryption-decryption etc. can now use the integrated GPGPU to deliver better performance. Recent works have proposed allowing GPUs to invoke traditional operating systems paging mechanisms and hardware MMUs to fault pages \cite{tlb-translation}. This provides the added benefit to be able to run GPU programs whose dataset sizes are not constrained by the memory size. As IHS platforms gain widespread adoption in HPC systems, improving the performance of these platforms will become of paramount importance in the near future. \cite{apu-exascale,amd-exascale1}


\section{3D Die-stacked DRAM}

\begin{figure}[!htb]
	\centering
	\def\svgwidth{0.7\columnwidth}
	\input{figures/stackedDRAM.pdf_tex}
	\caption{Proposed architecture of IHS with DRAM Stacking}
	\label{fig:stackdram}
\end{figure}
\par The advent of die-stacking technology \cite{3d-stacking} provides a way to integrate disparate silicon die of NMOS DRAM chips and CMOS logic chips with better interconnects. The implementation is accomplished either by 3D vertical stacking of DRAM chips using through-silicon vias (TSV) interconnects or horizontally/2.5D stacking on an interposer chip as depicted in Figure \ref{fig:stackdram}. This allows the addition of a sizable DRAM chip of capacities ranging from a couple of hundreds of megabytes to a few gigabytes close to the processing cores. These stacked DRAM devices provide high bandwidths of close to 400GB/s compared to the 90GB/s of DDR4 bandwidth \cite{xeonphi}. The better interconnect also lowers the latency of access compared to off-chip memory \cite{alloy}. 
\par Several recent proposals advocate the use of on-chip DRAM capacity as a hardware managed last level cache for improving performance of multicore CMPs \cite{alloy,bimodal,loh-hill,atcache}. In the context of IHS architecture, the stacked DRAM can cater to the large bandwidth requirements of throughput-oriented GPUs while the latency-sensitive CPU applications can benefit from reduced latency of data access. 
This also reduces energy consumed per access for the overall system, in line with the goals of IHS.
\par Managing contention for shared DRAMCache in the memory hierarchy of the two heterogeneous processor architectures which have asymmetric sensitivity and demands, however, introduces novel challenges. When a GPU kernel is launched it creates large number of concurrent threads which run in lock-step SIMD execution model and sends a large number of requests into the memory hierarchy causing congestion. This causes bottlenecks in request queues at the DRAMCache thus severely hampering CPU performance. Further, GPUs are designed to tolerate longer memory latencies while the memory hierarchy of typical CPUs have been designed to optimize memory access latencies for CPUs. 
Thus latency-sensitive CPU applications would benefit from a DRAMCache design that offers lower hit-time and hence a a lower miss penalty for the previous level caches (L2 cache). On the other hand, the GPU cores would benefit from higher bandwidth and higher hit-rates in DRAMCache even at the cost of higher hit-time. This necessitates the need for a careful design of memory system for IHS processors, that can handle the GPGPU bursty behavior as well as service requests for the CPU with a consistent latency without idling resources while delivering improved system throughput at lower energy.