\chapter{Introduction} \label{chap:introduction}

\par The remarkable improvements in computing power of the modern microprocessor over the last few decades can predominantly be attributed to shrinking of feature sizes. Miniaturization of transistors has allowed addition of specialized on-chip hardware circuitry for acceleration units \cite{accelerators-on-chip}.
A study in 2010 by Koomey et. al \cite{koomey} found that the amount of computation that could be done per unit of energy doubled about every 18 months. However, to reach exascale and beyond requires a thousand fold decrease in energy consumed per flop computed.
Graphics processing units (GPUs) have evolved from being fixed-function pipelines and are being used to accelerate data parallel code for general purpose (GP) applications. Compared with multi-core CPUs, GPGPUs offer the potential for better performance of data parallel code at lower energy. 
GPU have evolved into general purpose parallel processors allowing programming these devices using standard libraries and APIs such as OpenCL \cite{opencl}  and CUDA \cite{cuda}. These APIs have allowed writing programs that execute across heterogeneous platforms.
There has been considerable effort in recent years to develop GPU implementations of a wide variety of frameworks \cite{megha,fluidcl} and parallel algorithms from High Performance Computing (HPC) and the enterprise domains \cite{parboil,gpu-bfs}. GPGPU architectures and compiler transformations have also evolved over several generations and are now able to efficiently execute programs with irregular branches and memory behaviour as well. 
\par However, several hurdles remain to create a productive programming environment in heterogeneous systems where the GPU accelerator is present as a discrete device. Traditionally these discrete GPUs have their own independent memory systems and the CPU must copy data to the GPUs memory and back. This data movement is wasteful is expensive in terms of both latency and energy. Further, as the transfer happens over a slower PCIe bus, the separate address spaces and the need to manage two sets of data complicates programming, impeding expansion of the workloads that benefit from GPGPU computing.


\section{Integrated Heterogeneous Systems (IHS)} \label{ihs-intro}

\begin{figure}[!htb]
	\centering
	\def\svgwidth{\columnwidth}
	\input{figures/ihs-schematic.pdf_tex}
	\caption{A High Level Schematic of IHS Architecture}
	\label{fig:ihs-schematic}
\end{figure}
\par Recently, modern processor chips (e.g., AMD APUs \cite{amd-apu}, Intel Iris \cite{inteliris}, and NVIDIA Denver \cite{denver}) have heterogeneous processors which integrate CPU and GPU on the same die as shown in figure \ref{fig:ihs-schematic}. In doing so, these IHS architectures address some of the obstacles for programmability of these GPU devices. Specifically, the IHS architectures support a fully shared virtual and physical memory address space. This greatly simplifies programming as it provides the following features.
\begin{itemize}
	\item Discrete GPUs required programmers to identify data that would be operated on by the GPU. These would then have to be explicitly copied to GPU memory. IHS architectures feature a physical memory shared with GPUs. Hence, the GPU now touches only the data required by the application, removing the programmer burden of identifying the required data structures.	
	\item In discrete GPUs, operating on data structures with pointers such as linked lists, trees, graphs etc. required special handling by the programmer (such as deep copy) which impeded porting applications to GPU. With IHS, GPU can dereference and process pointers shared with CPU. Thus, the GPU can operate on the same application data structures as the CPU.
	\item In discrete GPUs, when CPUs and GPUs access the same memory region \cite{uva}, the graphics driver is required to flush and invalidate the GPU caches in order for CPU and GPU to share results. However, IHS provides a fully coherent shared memory similar to multi-core CPUs. This enables authoring applications of the produce-consumer paradigm that closely couple the CPU and GPU executions.
	\item Traditional discrete GPU have strictly limited support for multi-process offloading. They either resort to hard partitioning of GPU cores or serialize executions of programs. IHS processors allow for Read/Write protection to be enforced by MMU units and TLBs. This allows support for preemption and content switching to maintain high throughput by co-scheduling in a multi-process environment.
	\item GPU can invoke operating system kernel routines for demand paging and page faults with much lesser overheads \cite{tlb-translation}. This provides the benefit to be able to run GPU programs whose dataset sizes are not constrained by the memory size. 
	\item The HSA foundation \cite{hsafoundation} was setup to develop and define cross-vendor hardware specifications and software development tools needed to allow applications software to better use IHS architectures. HSA provides additional features such as the runtime system on IHS architectures. The HSA runtime provides support for "platform atomics" like Read Modify Write (RMW) operations as opposed to the atomics provide on a discrete PCIe based GPU which were restricted to PCIe provided atomics. 
	\item Recent HSA runtimes \cite{hsa-queuing} have proposed user-level command queuing. This features allows user mode process to dispatch directly into those GPU queues, requiring no OS kernel switching or driver overheads. This greatly reduces GPU initialization time.
\end{itemize} 

The above benefits enable several high level languages \cite{sumatra,julia} to also take advantage of the parallel processing synergistically with the CPU. Programmers can now write applications that seamlessly integrate CPUs with GPUs while benefiting from best attributes of each. Fine-grain parallel stream processing like face detection, compression, encryption-decryption etc., can now use the integrated GPGPU to deliver better performance.  
\par To summarize, integrating the GPUs on chip reduces the GPU programming barrier, allows for low overhead communication between CPU and GPUs, improved energy efficiency, low overhead coherence and synchronization between CPU and GPU and thus improved performance.




\section{3D Die-Stacked DRAM} \label{stacked-dram-intro}

\par In contrast to the processing capabilities of the modern microprocessor, DRAM memory speeds have not kept pace commensurately to serve the increasing demands of the processors. As this gap widens and processors get more capable by packing faster and larger numbers of cores, their performance will be limited by the ability to feed information into them. The speed imbalance coupled with a limited growth in pin counts has led to the memory and bandwidth wall \cite{memory-wall,bandwidth-wall} for off-chip DRAM systems which often becomes a performance limiting factor.
%The other key challenge is the requirement of memory bandwidth to keep the cores running without stalls. For this, each DDR generation usually increases the N-bit prefetch architecture (doubling the number of bits are fetched from the DRAM core) with each successive DDR generation without any change to the core of the DRAM. The implication of this is that processors now need to double the amount of data that they read and write in each operation. This scaling trend is eventually limited by the cache line sizes. 
Newer emerging memory technologies like 3D die-stacking of DRAM provide alternatives to address some of these challenges. These devices provide large number of channels using TSVs (though-silicon-via) and have lower signalling delays. These die stacked DRAMs are touted to play an important role in allowing the scaling of multi-core and IHS processors.
However, to be effective these DRAMs require careful architecting of the plethora of design parameters. These would include but are not limited to design decisions made in the context of traditional off-chip DRAM organizations that would require to be revisited to factor the architecture of these 3D die-stacked DRAMs. These design decisions are also often interlinked with workload characteristics and processor architectures.

\begin{figure}[!htb]
	\centering
	\def\svgwidth{\columnwidth}
	\input{figures/stackedDRAM.pdf_tex}
	\caption{Proposed Architecture of DRAM Stacking}
	\label{fig:stackdram}
\end{figure}
\par The advent of die-stacking technology \cite{3d-stacking} provides a way to integrate disparate silicon die of NMOS DRAM chips and CMOS logic chips with better interconnects. The implementation is accomplished either by 3D vertical stacking of DRAM chips using through-silicon vias (TSV) interconnects or horizontally/2.5D stacking on an interposer chip as depicted in Figure \ref{fig:stackdram}. This allows the addition of a sizeable DRAM chip of capacities ranging from a couple of hundreds of megabytes to a few gigabytes close to the processing cores. These stacked DRAM devices provide high bandwidths of close to 400GB/s compared to the 90GB/s of DDR4 bandwidth \cite{xeonphi}. The better interconnect also lowers the latency of access compared to off-chip memory \cite{alloy}. Several independent studies regarding roadmaps to exascale computing \cite{apu-exascale,amd-exascale1} also concur that die-stacked memory technology is a necessary cog to achieve the exascale bandwidth requirements with the targetted energy efficiency. 
\par Principally, there have been two schools of thoughts to organize stacked DRAMs in the memory hierarchy. The first is to use it as a part-of-memory \cite{pom,cameo}. These organizations deal with issues relating determination and appropriate allocation/placement of data in the stacked DRAM. Stacked DRAMs as part-of-memory allows using this capacity to increase the physical addressable memory available to the processor. Operating systems memory management and/or various software policies can also be used to allocate and page data in and out of the stacked DRAMs \cite{software-dram}.
\par The second approach advocates the use of on-chip DRAM capacity as a hardware managed last level cache for improving performance of multicore CMPs \cite{alloy,bimodal,loh-hill,atcache,footprint}. This also reduces energy consumed per access for the overall system.
However, stacked DRAM require careful designing since the large cache size (in the order of hundreds of megabytes to few gigabytes) implies the total size of tags associated with it can also be quite large (order of megabytes). The large tags storage requirement has created a space/time trade-off problem. On the one hand, we would like the latency of a tag access to be low as it contributes to both the hit latency and miss latency and hence store it in a SRAM structures (tags-in-SRAM). This SRAM storage space becomes prohibitive. Accordingly proposals have suggested storing tags in the stacked DRAM (tags-in-DRAM) alongside the data and to make it practical these schemes propose compound access \cite{loh-hill} or multiple bursts \cite{alloy} to retrieve tags efficiently. To reduce miss latency these works propose predictors with a few kilobytes of SRAM overhead to reduce miss latency.



\section{A Stacked DRAMCache for IHS Architectures}
To improve the memory subsystem of IHS and thus enhance its performance, we introduce a large capacity stacked DRAM, used as a hardware managed cache, as the  first level shared resource in the memory hierarchies of CPU and GPU cores. There have been studies and proposals to share the on-chip last-level SRAM caches \cite{helm,tap} and non-volatile STT-RAM caches \cite{oscar} in an IHS architectures. The work in HeLM \cite{helm} and TAP \cite{tap} address the issues of SRAM cache partitioning strategies between these processors in IHS architectures. OSCAR \cite{oscar} focuses on optimizations for non-volatile STT-RAM based caches which have asymmetric read/write latencies and network-on-chip optimizations for IHS architectures. To the best of our knowledge this is the first work that architects stacked DRAMCaches for IHS architectures.
\par In the context of IHS architecture, the stacked DRAM can cater to the large bandwidth requirements of throughput-oriented GPUs while the latency-sensitive CPU applications can benefit from reduced latency of data access. Although, adding the DRAMCache naively to IHS architecture improves performance, yet it leaves significant performance on the table.  The reason for this is the  disparate demands from CPU and GPU cores for DRAMCache and memory accesses. When a GPU kernel is launched it creates a large number of concurrent threads which run in lock-step SIMD execution model and sends a large number of requests into the memory hierarchy causing congestion. This causes bottlenecks in request queues at the DRAMCache thus severely hampering CPU performance. This imbalance can significantly reduce the performance benefits that the CPU cores would have otherwise enjoyed with the introduction of the DRAMCache, necessitating a heterogeneity aware management of this shared resource for improved performance. 
\par Further, GPUs are designed to tolerate longer memory latencies while the memory hierarchy of typical CPUs have been designed to optimize memory access latencies for CPUs. Thus latency-sensitive CPU applications would benefit from a DRAMCache design that offers lower hit-time and hence a lower miss penalty for the previous level caches (L2 cache). On the other hand, the GPU cores would benefit from higher bandwidth and higher hit-rates in DRAMCache even at the cost of higher hit-time.
This necessitates the need for a careful design of memory system for IHS processors, that can handle the GPGPU bursty behaviour as well as service requests for the CPU with a consistent latency without idling resources while delivering improved system throughput at lower energy.
\par Our organization, \cachename, is aware of the asymmetric and contrasting requirements from the heterogeneous processors. \cachename\ attempts to meet CPUs requirement of reduced hit times and lower miss penalties while at the same time improving hit rates for GPU to allow it to better use the DRAMCache bandwidth. In the common case when CPU is running alone, \cachename\ is an aggressive direct mapped DRAMCache optimized for hit times and lower miss penalty through hit/miss prediction as in \cite{alloy}. 

\par However, when GPU is active we propose (i) PrIS - a DRAMCache controller scheduling algorithm to prioritize scheduling of CPU requests in the queues to reduce the large waiting time caused due to burst of GPU requests (ii) ByE - a mechanism to allow some of the CPU requests (access requests that are clean data in DRAMCache or data that is currently not in DRAMCache) to be temporally bypassed to utilize the under-utilized off-chip memory bandwidth (iii) Chaining - a technique to force spatial occupancy control by providing a pseudo associativity for GPUs to improve hit rates while still allowing certain guaranteed occupancy for CPU lines.

\cachename\ achieves these goals using a lightweight and dynamic scheme which does not impose hard partitions on the shared DRAMCache. To evaluate
\cachename, we extend the gem5-gpu \cite{gem5-gpu} by adding the stacked DRAM as a memory-side hardware managed cache.

Using detailed simulations, we show that the techniques proposed in the thesis  help to effectively address the disparate demands of IHS
processors and improve performance of both CPU and GPU cores by 211\% and 20\% respectively, over a IHS processor with no DRAMCache.  Further the proposed optimizations improve overall system performance, on an average by 41\% over a carefully designed, but heterogeneity unaware DRAMCache. 
\par Further, we show that our solution, \cachename, is robust and performs well even with a wide variety of system configurations. \cachename\ performs better with larger capacity DRAMCache and latest generations of aggressive off-chip DRAMs compared to a naive DRAMCache. We also show \cachename\ scales well when operating with projected stacked DRAMs of higher bandwidths, outperforming a naive DRAMCache for CPU applications and GPU applications.
We also show that \cachename\ performs better than the state-of-the-art DRAM scheduling scheme for IHS architectures i.e., Staged Memory Scheduler \cite{sms} and state-of-the-art DRAMCache bypass technique - Mostly Clean DRAMCache \cite{mostly-clean}.

\par To summarize, the thesis makes the following contributions:
\begin{itemize}
	\item We quantify the impact of a shared physical memory in IHS architectures and motivate the need for a better memory subsystem. We argue that a die stacked DRAMs, used as a hardware managed cache, can be a suitable fit for the memory system requirements of IHS architecture. Further, we show that a naive heterogeneity unaware DRAMCache can be suboptimal.
	\item We design an effective DRAMCache organization that is aware of this inherent heterogeneity in IHS processors. Beside tailoring the organization of the DRAMCache itself, we propose 3 heterogeneity aware mechanisms, \prioname, \bypassname\ and \chaining\ to improve the performance of the DRAMCache.
	\item To evaluate \cachename, we develop simulator modules and extend gem5-gpu simulator to simulate IHS architectures with stacked DRAM as cache.
	\item We demonstrate using detailed simulation of co-running CPU and GPU workloads that, compared to a naive DRAMCache, the proposed \cachename\ solution significantly improves the performance of CPU cores albeit with a small reduction in performance of GPU cores. Further, \cachename\ outperforms a naive DRAMCache on all IHS system performance metrics.
	\item We show that \cachename\ is robust and scales well even for projected memory system configurations. We also compare performance of our \cachename\ against state-of-the-art DRAM/DRAMCache proposals, namely Staged Memory Scheduling \cite{sms} and Mostly Clean DRAMCache \cite{mostly-clean} and demonstrate that \cachename\ outperforms these schemes.
\end{itemize}

\section{Thesis Organization}
The rest of the thesis is organized as follows. In Chapter \ref{chap:background}, we present the necessary background and motivation of this work. 
Chapter \ref{chap:hashcache} details the design and mechanisms of \cachename.
Chapter \ref{chap:simulator} elucidates our simulation infrastructure, the workloads used for evaluation, methodology and the IHS performance metrics used to evaluate the various schemes proposed. 
In Chapter \ref{chap:results}, we report performance of \cachename, demonstrate its sensitivity and scaling properties. Wwe also compare \cachename\ mechanisms by adapting some of the state-of-the-art in the area. Chapter \ref{chap:related-work} provides a discussion of the related work in DRAMCaches, IHS architectures and the IHS Benchmarks. Chapter \ref{chap:conclusion} summarizes our work and notes a few potential future directions.