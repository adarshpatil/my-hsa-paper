\chapter{Introduction}
\label{chap:introduction}

%\begin{quote} \small



\par The remarkable advances in computing power of the modern microprocessor over the last few decades can predominantly be attributed to shrinking of feature sizes. Miniaturization of transistors has allowed addition of specialized on-chip hardware circuitry for acceleration units. 
A study in 2010 by Koomey et. al \cite{koomey} found that the amount of computation that could be done per unit of energy doubled about every 18 months. However, to reach exascale and beyond requires a thousand fold decrease in energy consumed per flop computed.
Graphics processing units (GPUs) have evolved from being fixed-function pipelines and are being used to accelerate data parallel code for general purpose (GP) applications. Compared with multi-core CPUs, GPGPUs offer the potential for better performance of data parallel code at lower energy. 
There has been considerable effort in recent years to develop GPU implementations of a wide variety of parallel algorithms from the High Performance Computing (HPC) and the enterprise domains. GPGPUs architectures and compiler transformations have evolved over several iterations and are now able to efficiently execute programs with irregular branches and memory behaviour as well. 
Traditionally these discrete processors have had their own independent memory systems. To take advantage of the discrete GPUs, the CPU must copy data to the GPUs memory and back. This data movement is wasteful and expends energy while also adding latency as the transfer happens over a slower PCIe bus. The separate address spaces and complex programming models that need to manage two sets of data further impede expansion of the workloads that benefit from GPGPU computing. 

\par In contrast to the processing capabilities of the modern microprocessor, DRAM memory speeds have not kept pace commensurately to serve the increasing demands of the processors. This speed imbalance coupled with a limited growth in pin counts has led to the memory and bandwidth wall \cite{memory-wall,bandwidth-wall} for off-chip DRAM systems which often becomes a performance limiting factor. The key challenge in large multi-core processors is the requirement of memory bandwidth to keep the cores running without stalls. For this, each generation usually increases memory channels with each DDR generation, which increases memory system power consumption.
A typical DRAM device organization contains banks where each bank contains a 2D array of storage cells, row and column decoders, sense amplifiers and peripheral circuits. Banks are further organized into ranks which share command busses but have separate data busses. A chip select is used to select a bank to issue a command. Commercially DRAMs are available as DIMM (Dual in-line memory module) containing several DRAM devices with a standard connector. A memory controller controls the operation of the DRAM device while respecting timing constrains and bus conflicts.


%\end{quote}

\section{Intergrated Heterogenous Systems (IHS)} \label{ihs-intro}
\begin{figure}[!htb]
	\centering
	\def\svgwidth{0.7\columnwidth}
	\input{figures/arch.pdf_tex}
	\caption{Architecture of an Integrated Heterogeneous System}
	\label{fig:hsa-arch}
\end{figure}

\par Modern processor chips have heterogeneous processors which integrate a lower capacity GPU on-die allowing for graphics rendering only. In view of the widespread use of the GPU for general purpose applications, processor manufacturers including AMD\cite{amd-apu}, Intel\cite{inteliris}, and NVIDIA\cite{denver} are beginning to allow general purpose OpenCL\cite{opencl}/CUDA\cite{cuda} programs to run on their Integrated Heterogeneous System (IHS) platform which were so far restricted only to graphics rendering. The HSA Foundation \cite{hsafoundation} was setup to develop and define cross-vendor hardware specifications and software development tools needed to allow application software to better use this IHS architecture. 

\par The IHS architecture, as illustrated in Figure \ref{fig:hsa-arch}, consists of multiple CPU cores with private L1 and shared L2 caches along with multiple GPU cores or Compute Units (CUs) with private L1 and shared L2 cache. These caches (except GPU L1 caches) are kept coherent with a heterogeneous MOESI-like coherence protocol. The Network-on-chip (NoC) connects the cores with the caches and the memory controllers, while the memory itself is off-chip and connected via memory channels. 
\par The IHS architecture supports a shared virtual address space making pointer sharing semantics possible between CPU and GPU which simplifies programming. Further, a shared physical address space and the cache coherent interconnect reduces GPU initialization time and enables several high level languages \cite{sumatra,julia} to also take advantage of the parallel processing synergistically with the CPU. Programmers can now write applications that seamlessly integrate CPUs with GPUs while benefiting from best attributes of each. Fine-grain parallel stream processing like face detection, compression, encryption-decryption etc. can now use the integrated GPGPU to deliver better performance. Recent works have proposed allowing GPUs to invoke traditional operating systems paging mechanisms and hardware MMUs to fault pages \cite{tlb-translation}. This provides the added benefit to be able to run GPU programs whose dataset sizes are not constrained by the memory size. As IHS platforms gain widespread adoption in HPC systems, improving the performance of these platforms will become of paramount importance in the near future. \cite{apu-exascale,amd-exascale1}


\section{3D Die-stacked DRAM} \label{stacked-dram-intro}

\begin{figure}[!htb]
	\centering
	\def\svgwidth{0.7\columnwidth}
	\input{figures/stackedDRAM.pdf_tex}
	\caption{Proposed architecture of IHS with DRAM Stacking}
	\label{fig:stackdram}
\end{figure}
\par The advent of die-stacking technology \cite{3d-stacking} provides a way to integrate disparate silicon die of NMOS DRAM chips and CMOS logic chips with better interconnects. The implementation is accomplished either by 3D vertical stacking of DRAM chips using through-silicon vias (TSV) interconnects or horizontally/2.5D stacking on an interposer chip as depicted in Figure \ref{fig:stackdram}. This allows the addition of a sizable DRAM chip of capacities ranging from a couple of hundreds of megabytes to a few gigabytes close to the processing cores. These stacked DRAM devices provide high bandwidths of close to 400GB/s compared to the 90GB/s of DDR4 bandwidth \cite{xeonphi}. The better interconnect also lowers the latency of access compared to off-chip memory \cite{alloy}. 
\par Several recent proposals advocate the use of on-chip DRAM capacity as a hardware managed last level cache for improving performance of multicore CMPs \cite{alloy,bimodal,loh-hill,atcache,footprint}. In the context of IHS architecture, the stacked DRAM can cater to the large bandwidth requirements of throughput-oriented GPUs while the latency-sensitive CPU applications can benefit from reduced latency of data access. This also reduces energy consumed per access for the overall system, in line with the goals of IHS.
However, stacked DRAM require careful designing since the large cache size (in the order of hundres of megabytes to few gigabytes) implies the total size of tags associated with it can also be quite large (order of megabytes). The large tags storage requirement has created a space/time trade-off problem. On the one hand, we would like the latency of a tag access to be low as it contributes to both the hit latency and miss latency and hence store it in a SRAM structures (tags-in-SRAM). This SRAM storage space becomes prohibitive. Accordingly proposals have suggested storing tags in the stacked DRAM (tags-in-DRAM) alongside the data and to make it practical these schemes propose compound access \cite{loh-hill} or multiple burts \cite{alloy} to retrieve tags efficiently. To reduce miss latency these works propose predictors with a few kilbytes of SRAM overhead to reduce miss latency.

\section{Motivation} \label{motivation}
\par Managing contention for shared DRAMCache in the memory hierarchy of the two heterogeneous processor architectures which have asymmetric sensitivity and demands, however, introduces novel challenges. When a GPU kernel is launched it creates large number of concurrent threads which run in lock-step SIMD execution model and sends a large number of requests into the memory hierarchy causing congestion. This causes bottlenecks in request queues at the DRAMCache thus severely hampering CPU performance. Further, GPUs are designed to tolerate longer memory latencies while the memory hierarchy of typical CPUs have been designed to optimize memory access latencies for CPUs. 
Thus latency-sensitive CPU applications would benefit from a DRAMCache design that offers lower hit-time and hence a a lower miss penalty for the previous level caches (L2 cache). On the other hand, the GPU cores would benefit from higher bandwidth and higher hit-rates in DRAMCache even at the cost of higher hit-time. This necessitates the need for a careful design of memory system for IHS processors, that can handle the GPGPU bursty behavior as well as service requests for the CPU with a consistent latency without idling resources while delivering improved system throughput at lower energy.
As mentioned in section \ref{ihs-intro}, the large capacity provided by the stacked DRAM is in line with the working set requirements of IHS processors. Using this capacity as a hardware managed cache could provide performance gains without any application modifications. The bandwidth benefits and modestly improved latency provided by the DRAMCache help improve performance of IHS processors over on-chip SRAM caches of reasonable sizes \cite{amd-exascale1}. 
\begin{figure}[htbp]
	\centering
	\includegraphics[scale=1]{graphs/motivation-cpu}
	\includegraphics[scale=1]{graphs/motivation-gpu}
	\caption{Performance comparison of (a) CPU (b) GPU when running alone, co-run, with and w/o DRAMCache}
	\label{fig:motivation}
\end{figure}
\par In this work, we assume a cache organization similar to Alloy cache \cite{alloy} with a block size of 128 bytes and study the problems and challenges in using the DRAMCache for IHS architectures. We justify the design decisions in the section \ref{section-hashcache}. 
Each CPU core has a private L1 cache and a shared split L2 cache across the CPU cores. The GPU cores have private L1 and shared L2 cache among themselves.  
The stacked DRAMCache (of size 64MB) is the first level shared cache across CPU cores and GPU cores. In our experiments, we consider a multi-programmed workload on the CPU cores and a GPGPU application on a single CPU core and multiple CUs 
\footnote{It should be noted here that the GPU application has a CPU component and alternates execution on CPU core and the CUs (kernel execution)}. 
We refer to this workload as Heterogeneous Application (HeA), as the CPU and GPU applications are co-run. We use the terms Homogeneous Application (HoA-CPU and HoA-GPU) to refer to the cases when (multiple) CPU applications are run alone and GPU application is run alone. 
%Our study on the interference due to co-running CPU and GPU application reports performance numbers of the GPU application when the GPU kernel is running on the CUs.
Additional details relating to the experimental methodology and workloads are described in Section \ref{methodology}


\begin{figure}[htb]
	\centering
	\includegraphics[scale=1]{graphs/motivation-cpu-cache}
	\caption{DRAMCache CPU Access Latency \& Hit rate}
	\label{fig:motivation-cpu-cache}
\end{figure}
First, we evaluate the benefits of having a large DRAMCache for the CPU in an IHS architecture. Figure \ref{fig:motivation}(a) presents the performance improvement for CPU cores due to the addition of a stacked DRAMCache (in terms of harmonic mean of IPCs, normalized to HeA without a DRAMCache) when co-run with a GPU and when run alone.
% that is obtained by adding a stacked DRAMCache when co-run with the GPU application and also when run alone. 
As can be seen from the figure, the addition of a stacked DRAMCache improves performance of CPU applications in an IHS architecture, on an average by just 42\%. However, when the CPU runs alone with a stacked DRAMCache it achieves a 3.72x better performance than that achieved in IHS without a DRAMCache. 
%In other words, the performance gain by adding a DRAMCache is on an average 2.6x lesser in IHS. 
We observe that this huge performance gap is primarily due to the interference of GPU applications.
Although this HoA-CPU performance cannot be achieved in IHS there still exists a significant opportunity for improvement.
This gap in performance can be attributed to the unmanaged heterogeneity and interference in the DRAMCache organization.
\par We further investigate the cause of this performance gap. Figures \ref{fig:motivation-cpu-cache} presents the DRAMCache access latency experienced by a CPU request (in terms of CPU cycles) on the primary y-axis and the CPU hit rates on the secondary y-axis, while running alone and co-run with the GPU application. We find that the presence of GPU application increases the average access latency of the DRAMCache by 213\% while hit rates of the CPU are marginally impacted (only by about 4\%) when co-running.
This increase in average access latency is primarily attributed to the large
number of GPU requests flooding the DRAMCache controller when the GPU kernels are co-running with the CPU applications.
It should be noted here that, even though we use the terms CPU and GPU requests separately, they may refer to the same data. This terminology only indicates the source of the request. 
%We also note that the GPGPU application allows some amount of sharing of data within the DRAMCache between the CPU and GPU processors (i.e. constructive interference - blocks brought in by the CPU and then used by the GPU and vice versa).
\par Next, we study the impact of co-running on the GPU applications. Figure \ref{fig:motivation}(b) presents GPU performance obtained by addition of a stacked DRAMCache when co-run with the CPU and when run alone.
We observe that with the introduction of DRAMCache in an IHS, the GPU performance improves on an average by 24\%. This performance is within 10\% of its HoA GPU performance with DRAMCache. 
Thus, co-running with CPU applications has only a minor impact on the GPU performance. 
%Some cache-sensitive GPU workloads show benefits from the addition of the DRAMCache while other workloads, that do not significantly reuse cached data, show no performance benefits.
%\begin{figure}[htb]
%   \centering
%   \includegraphics[scale=0.8]{graphs/motivation-gpu-cache}
%   \caption{GPU Miss reduction with 2-way associativity}
%   \label{fig:motivation-gpu-cache}
%\end{figure}
%\par Additionally, we observe that providing associativity in DRAMCache reduces the number of GPU misses going to the relatively constricted off-chip DRAM bus. 
%Figure \ref{fig:motivation-gpu-cache} plots the reduction in number of GPU misses in log scale, by introducing 2-way associativity over our direct mapped DRAMCache. The GPU misses reduce by an average of $10^5$ for 2-way associative cache with half the number of sets and $10^6$ for a 2-way associative cache with the same number of sets.
%In comparison, the total number of CPU read requests to the DRAM are in the order of $10^7$. This considerable reduction in GPU misses leads to lower congestion for CPU requests at the DRAM. While associativity leads to only a modest improvements in hit rates for the GPU, it avoids incurring double queuing latency for all the reduced GPU misses. This corroborates the observation by other works \cite{oscar} that the number of requests from the GPU are orders of magnitude higher than CPU.
%Thus, providing some associativity can increase DRAMCache bandwidth utilization and improve the system performance.

While the above discussion indicate that the design decisions of heterogeneity aware DRAMCache are influenced by the latency-sensitive CPU applications, we emphasize the need to effectively utilize the large capacity and the higher bandwidth of DRAMCache by GPU applications. Towards this goal, we experimented with a DRAMCache of 128MB and 2-way associativity. We observe that the hit rate for GPU applications improves, on an average, by 5\%. While the improvement in hit-rate is not significant, as the number of GPU requests is very large (2 or 3 orders of magnitude higher than the CPU requests), even such a small increase in the hit rate leads to large improvements in utilization of the DRAMCache bandwidth by the GPU application. This makes a case for improving the associativity for GPU requests by introducing some pseudo-associativity in the DRAMCache, without impacting the hit-time offered by a direct-mapped tag-and-data (TAD) cache.

\par Based on the above motivation study we conclude that there are significant performance benefits that could be obtained 
with the introduction of the stacked DRAMCache for the latency-sensitive CPU applications. However, in a naive implementation
of the DRAMCache, these benefits can be lost due to interference from the co-running GPU application.  Hence it is important to carefully 
architect the DRAMCache organization to ensure CPU applications are not hampered due to this interference.
This requires the design to be aware of the heterogeneity of the applications (CPU vs GPU) and their demands on the 
memory hierarchy.  Hard partitioning of DRAMCache would not be an effective design as it leads to under-utilization of the large capacity 
stacked DRAM, as the effects of interference due to co-running GPU application is felt only when the kernel is run on the GPU sporadically. 
Further, effectively utilizing the under-utilized main memory bandwidth \cite{micro-refresh, mainak-hpca, bear}
is also important to achieve higher performance.  Lastly, the design should ensure that the CPU applications and the 
GPU application are able to utilize the large capacity of the stacked DRAM effectively to meet the working set 
requirements of the respective applications in the best possible way.

\section{Contributions}
This thesis makes the following contributions
\begin{itemize}
	\item Studies the impact of co-running in a IHS architecture on the CPU and GPU
	\item Develops simulation methodology and simulator modules for studying DRAMCaches in IHS workloads
	\item 
\end{itemize}

\section{Thesis Organization}

