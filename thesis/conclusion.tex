\chapter{Conclusions} \label{chap:conclusion}
In this chapter, we summarize the \cachename\ solution and discuss potential avenues for future work.
\section{Summary}
Stacked DRAM caches require careful design to be able to improve performance of workloads. There are several conflicting factors that require careful consideration and trade-offs. This is compounded by the fact that in IHS architectures the processors themselves are  heterogeneous and have disparate requirements from the DRAM cache i.e., latency for CPUs vs throughput for GPUs.
\par In this work, first we demonstrated that current memory hierarchy requires to be revisited for IHS architectures to be able to perform upto capacity. We presented a case for performance improvement of an IHS processors by addition of a stacked DRAM cache. 
We quantify the effects of interference due to co-running on each processor and show that the heterogeneity adversely effects CPU performance as compared to the GPU. 
Consequently, we carefully design an effective DRAM cache organization for IHS processors by evaluating the design space of DRAM caches.
Further, we propose three simple and effective heterogeneity-aware techniques --- a heterogeneity-aware DRAM cache scheduler (\prioname), a heterogeneity-aware temporal bypass (\bypassname) and a heterogeneity-aware spatial occupancy control (\chaining) scheme to improve the performance IHS architectures.
\par We develop the simulation modules and infrastructure to simulate an IHS architecture with a DRAM cache. We use a suitable methodology for detailed simulation of \cachename and adapt metrics to measure performance of the proposed system. Using this experimental setup we show that 
\cachename\ achieves significant improvement of 100\% in overall system performance on an average over a baseline system with no DRAM cache and 41\% over a heterogeneity unaware DRAM cache. 
We also show that \cachename\ mechanisms scale very well with stacked DRAMs of larger capacity, higher bandwidth and even with access latencies of off-chip DRAMs.
This work, thus shows that there are significant benefits of using a stacked DRAM cache for IHS processors, far exceeding the usefulness of such devices in homogeneous GPUs and multi-core CPU systems.

\section{Future Work}
An interesting direction for future work would be to explore differential granularity caching for IHS architectures in the stacked DRAM cache for CPU and GPU cores since GPU benchmarks have good spatial locality. Larger blocks with critical sub block first and early restart \cite{dram-book} consequently might prove beneficial as well. Prefetching GPU lines without overly polluting the cache would be another approach to serve GPU lines out of the DRAM cache thus better utilizing the large stacked DRAM bandwidth. However, such techniques need to be mindful that both the memory and cache are slower DRAM devices. 
\par Exploring ways of combining \prioname, \chaining\ and \bypassname\ as a unified technique is likely to improve performance. However, as the objectives of Chaining and ByE are somewhat conflicting, this requires careful consideration of replacement (fill) policies in the DRAM cache.
\par It would also be interesting explore the performance of \cachename\ mechanisms in Collaborative Heterogeneous Applications benchmarks (Chai) \cite{chai} where the CPU and GPU are used concurrently by the same application. DRAM cache management schemes using program hints about the collaboration patterns such as data partitioning, fine or coarse grain task partitioning would also be a interesting future work.
\par Since the IHS architectures use a shared virtual and physical address space, using TLBs to make the DRAM cache tagless might be a practical solution. However this requires careful consideration of the TLB reach and page walker state machine in IHS architectures.
