\section{Experimental Setup \& Methodology} \label{methodology}
We evaluate the performance of chaining using multi-programmed SPEC 2006 applications coupled with a Rodinia application that contains the GPU phase of execution. We use Rodinia \cite{rodinia} applications that are modified to elide the memcpy api calls to run with unified virtual and physical address spaces. These workloads are run on a cycle accurate simulator gem5-gpu \cite{gem5-gpu} which is configured to simulate cache coherent unified CPUs and GPUs using the VI\_Hammer protocol. The cache hierarchy has per SM private GPU L1  that are non-inclusive of the shared GPU L2 cache and can hold stale data. However, GPU L2 cache is coherent with all levels of the CPU hierarchy.
The DRAM Cache we evaluate here is the first level shared cache between the 2 split cache hierarchies of CPUs and GPUs while they have a shared level of cache within themselves.
We fast-forward the initialization phase of the workloads up until just before the launch of the first kernel of the GPU program. We ensure that each core executes atleast 2 Billion instructions and each 4 core workload executes 20 billion instructions and each 16 core workload executes x Billion on average in the fast-forward phase. We do this by letting adding no-ops to the Rodinia benchmarks for the duration until the initialization of the SPEC programs is complete.

Multiple memory controllers accesses are almost equally divided by clever XOR mechanism. Each mem ctrl is responsible for a 4GB memory DDR3 1600MHz DRAM and a 128MB HMC 2500MHz DRAM Cache device. The 128MB DRAM Cache caches only data from the 4GB memory chunk memory controller is responsible for. So there are no cross bus requests between controllers.

We faithfully model Fill Queue for fill requests for insertion into cache on the return path from main memory. \cite{dca}