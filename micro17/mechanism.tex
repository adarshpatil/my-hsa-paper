\newcommand{\bypassname}{\textit{ByE }}
\newcommand{\prioname}{\textit{PrIS }}
\subsection{\cachename Mechanism} \label{mechanism}
In this section, we propose three simple optimizations to improve the performance of IHS processors.

\subsubsection{Heterogeneity aware DRAMCache scheduling}
DRAM devices operate at a much lower clock rate compared to the processor cores and caches. Moreover DRAM cells tend to leak and have to be periodically refreshed to preserve the data which further reduces the available time for accessing the device. The imbalance in request arrival rate and service times creates a queuing effect. Hence DRAM devices have traditionally had limited size queues to hold requests until they can be serviced by the device. However, we observe that an IHS processor the large burst of requests from the GPU quickly exhausts the available queue positions (buffer locations) at the DRAMCache controller leading to requests being rejected for a retry. The CPU requests, which are interleaved with the GPU requests, are few and far spaced and thus suffer large waiting time due to retries. This is compounded by the fact that GPU exploits good row buffer locality and is preferentially scheduled by the DRAMCache controller (under the FR-FCFCS scheduling method \cite{sms}), causing increased queue latencies for CPU requests. Increasing queue lengths beyond a certain measure increases scheduling overheads as DRAM schedulers search the queues for the most suitable request to schedule based on a certain heuristics.
\par \cachename reduces waiting time of CPU requests by prioritizing them at the DRAMCache controller without starving GPU requests. 
For this, \cachename applies a CPU Prioritized FR-FCFS algorithm over each of the Read, Write and Fill queues to schedule a request at each bank. 
\prioname is cognizant of the request heterogeneity and searches the short queues for either a CPU row buffer hit request or a CPU row activation request to schedule before scheduling a GPU request in a FR-FCFS manner. For GPU requests, starvation is avoided firstly, by allowing GPU requests to be scheduled to a prepped row after the CPU has completed access to that row and secondly by allowing GPU to schedule its requests to a bank immediately, when the queue has no more CPU requests to that bank. These scheduling decisions are made subject to the device timing constraints, similar to an FR-FCFS scheduler.
\par Prioritization of CPU requests alone may not help, as the flood of memory requests from  GPU can quickly fill the precious buffer at the DRAMCache controller, resulting in CPU requests not even entering the buffer. HAShCache overcomes this problem by guaranteeing certain minimum occupancy for CPU requests in the buffer at the DRAMCache Controller. This is accomplished using  a selective reject-retry mechanism for GPU requests when the queues reach certain critical level.  Together these two mechanisms attempt to reduce the latency experienced by CPU requests.  We refer to these  mechanisms collectively as \prioname (CPU Prioritized FR-FCFS with IHS aware Scheduling). 
\par \prioname differentiates requests broadly as CPU or GPU requests and not within individual CPU cores or GPU SMs for scheduling. We find that in an IHS architecture the interference between CPU and GPU applications vastly overwhelms the interference between homogeneous application workloads. Hence, \prioname only has to make a binary selection for scheduling which greatly simplifying the scheduling algorithm. Hence, \prioname is a simple yet effective, single stage modified FR-FCFS algorithm that does not incur any hardware overhead in terms of multiple requests queues or batching stages as in \cite{sms}. This is accordance with our goal of a low energy DRAMCache.






\subsubsection{Heterogeneity aware Temporal bypass}
The large sizes of the stacked DRAMCache ensures cache lines have fairly long residency times before being evicted. Hence, DRAMCache have fairly large hit rates which leads to idling of off-chip DRAM bandwidth. Moreover the stacked DRAM and off-chip DRAM utilize the same underlying technology i.e. large arrays of charge retaining capacitor cells, and hence they incur similar access latencies to read and write to these capacitors.
In a IHS architecture, the increased access latency incurred by a CPU request at the DRAMCache (see Figure \ref{fig:motivation-cpu-cache}) when the GPU is running makes the DRAM an attractive target to direct some of the CPU requests. This leads to improved resource utilization without incurring any increased latencies for CPU requests. 
\par To hide the latency of miss, our aggressive baseline design already incorporates a hit/miss predictor (similar to MAP-I predictor \cite{alloy}) for CPUs. It initiates an early access to off-chip DRAM when it predicts a miss for the CPU requests in the DRAMCache. These requests are then enqueued in the DRAMCache queues for verification of a miss 
\footnote{This is required to ensure that a misprediction by the predictor does not result in using data from the DRAM for lines that are modified (dirty) in the DRAMCache}
by a tag match. 
When the tag is matched in the DRAMCache, in the case of a hit, the data from the DRAMCache is forwarded to the requestor and the DRAM memory access is squashed or its response is ignored. In the case of a miss, the data from the memory is forwarded and inserted into the cache. Normally, it is expected that the access to the DRAMCache completes earlier (due to its relatively lower access latency) than the DRAM response. However, when the GPU is running, the parallel request to DRAM memory often returns earlier and waits in the MSHRs. This is due to reduced queuing latencies at the DRAM compared to the DRAMCache.  HAShCache exploits this observation to bypass CPU read requests for both misses and clean lines.
\par For this \cachename uses a Bypass Enabler ( \bypassname). \bypassname uses a counting bloom filter \cite{bloom,counting-bloom} that tracks the dirty lines in the DRAMCache and provides a space efficient way to determine if a given request can be bypassed. The property of a bloom filter to answer "definitely not in set" allows us to bypass requests correctly i.e. without verifying tags in the set of the DRAMCache. 
On a write request when a cache line becomes dirty in the DRAMCache, the address is hashed into \bypassname and the corresponding counters are incremented. When a dirty line is evicted from the DRAMCache, \bypassname attempts to remove the entry from the Bloom filter by decrementing the corresponding locations \footnote{Counting bloom filters use saturating counters. If the counters saturates, decrementing the counter can lead to false-negative (dirty lines predicted as clean lines and wrongly bypassed). Hence saturated counters are never decremented.  While this may increase the false positives (clean lines being predicted as dirty), which only reduces the benefits obtained by ByE, it does not affect the functional correctness. In our implementation we observes that the 2-bit counters in the Bloom-filter saturates only  occasionally.}.

\begin{figure}[htb]
    \centering
    \bloom
    \caption{Working of \cachename with \bypassname }
    \label{fig:bye}
\end{figure}


\begin{figure*}[!htb]
    \centering
    \chainaccess
    \caption{\cachename (a)Row organization (2KB) (b) Access path}
    \label{fig:chain-access}
\end{figure*}
ByE bypasses CPU requests only when the GPU cores are executing the kernel. For this, all CPU read requests lookup into \bypassname as shown in Figure \ref{fig:bye}. If the Bloom filter search in \bypassname returns a  negative result the address is guaranteed to not be dirty in the DRAMCache. Thus, the request can safely be bypassed to utilize the off-chip DRAM bandwidth. 
Further, when the bypassed CPU requests return from the off-chip DRAM access, these requests are directly forwarded to the requester and are not inserted into the cache. Firstly, this allows \bypassname mechanism to ensure that future write requests for the line do not hit in the cache causing reduced bypass efficiency. Secondly, this  allows \bypassname to reduce some of the bloat caused by a Miss Fill \cite{bear} into the DRAMCache.
\par All write requests and GPU read requests proceed serially after looking into the cache. \bypassname does not bypass any write requests as it would otherwise require a back-invalidation of the cache line, if present in the DRAMCache, which would need a full DRAMCache access.
\par We find that a small 2-bit counting bloom filter implemented with two $H_3$ hash functions \cite{h3} and 512K entries per controller is sufficient 
to produce reasonable bypass efficiency with a tolerable mis-prediction rate. The total overhead for \bypassname is 256KB for a 64MB DRAMCache which is less than 0.4\% of the cache size.




\subsubsection{Heterogeneity aware Spatial Occupancy Control}
The schemes proposed in the previous two subsections, \prioname and \bypassname, attempt to improve the latency of CPU requests. The mechanism described in this section details \cachename's approach to improve the utilization of the DRAMCache for GPU requests, in order to exploit the higher bandwidth provided by it. We make the following observations and inferences: (i) For the GPU to be able to better utilize the DRAMCache bandwidth, the hit rates for GPU workloads should be large enough that the GPU does not have to frequently use the relatively constricted off-chip DRAM buses. (ii) As noted in Section \ref{motivation} GPUs can trade access latencies for higher hit rates. Further, providing associativity for GPU requests improves the hit rate (see Figure \ref{fig:motivation-gpu-cache}) (iii) The working sets of CPU applications tend to be limited to few tens of MBs due to the limited amount of MLP that can be exploited by the CPUs. Thus, providing larger than certain share of cache leads to no improvements in hit rates and IPC for CPU. Nevertheless, the CPU can still gain from some share of the DRAMCache due to reduced latency of access. (iv) Given that GPU can exploit much higher MLP using several thousands of threads, the relatively small GPU L2 cache provides limited filtering of traffic and has significantly high miss rates while on the other hand CPUs have sufficiently sized L2 cache sizes to be able to retain blocks longer before re-requesting a block.
The above observations lead to the following conflicting requirements. It is important to ensure that the CPU requests have certain share (minimum occupancy) in the DRAMCache to ensure the benefits of lower latency. To effectively use the larger share of DRAMCache for GPU requests, it may be required to increase  associativity of the DRAMCache. However such an associativity should not unduly increase the hit latency for CPU requests. 
\par To accomplish the above goals, \cachename uses a chaining scheme which introduces (pseudo) associativity mainly for GPU requests, while ensuring certain minimum occupancy for CPU requests. The chaining scheme uses a linear probing like technique inspired by the collision resolution mechanism of a hash map. 
To ensure minimum occupancy for CPU requests, chaining maintains a low-threshold value (\textit{$l_{cpu}$}) and when the occupancy of CPU lines
\footnote{As mentioned earlier we classify a data as CPU data or GPU data based on whose request last accessed the data in the DRAMCache, although the ownership may change subsequently.} 
reaches this threshold, chaining ensures GPU data does not replace data brought in by CPU. 
In the other situations, HAShCache modifies the replacement policy in the DRAMCache depending on the requesting core type. For a GPU request that is evicting another GPU line, \cachename looks for a line belonging to a CPU to replace within the same row in the next three consecutive locations.  
If $B$ is the original cache block, then the blocks considered for insertions are $(B+1)\%N_s$,$(B+2)\%N_s$, and $(B+3)\%N_s$, where $N_s$ is number of blocks in a DRAMCache row (page). Hence, the inserted block always lies in the same row as the original cache block. Note that for every set, there could be at most 1 chained set, providing a pseudo-associativity of at most 1.  
We refer to this inserted location as the \textit{chained block} and the actual cache block the request mapped to as the \textit{original block}. The location of the chained set is then represented as a 2-bit offset and is stored along with the metadata for the original set (see Figure \ref{fig:chain-access}(a)). When this cache block is evicted, if it was chained, to unchain it (from the original block), the offset is stored in the reverse chain bits field in  the metadata for the chained block.  
The chain dirty bit field (Figure \ref{fig:chain-access}(a)) in the metadata indicates whether the chained location, if any, holds modified data. This is used to optimize the access path for CPU and reduce the adverse effect of a double set lookup for latency sensitive CPU requests as shown in Figure \ref{fig:chain-access}(b). Chaining relies on the hit/miss predictor to have started an early access to memory and avoid the second set lookup for CPU if the parallel memory (PAM) access has returned and the chained block is clean. 
Additionally, each tag also stores 1 bit information about the owner of the block (CPU or GPU). This bit is used to make quick replacement decisions locally. The additional metadata required for chaining is only 6 bits which can easily be accommodated in the existing 8 byte metadata. Lastly, the unused 8 byte (at the end of each row(page)) is used to store ownership information of each block in the row (15 bits). This information is used to make the chaining replacement decision.

\par As explained earlier, this type of chaining continues only until the occupancy of the CPU blocks in the row reaches \textit{$l_{cpu}$}. Once this is reached, GPU lines are not allowed to evict a CPU line and such GPU requests contending to evict a CPU line in a block are forced to chain to another block belonging to a GPU and evicting that instead, thus maintaining the \textit{$l_{cpu}$} occupancy for CPU. In the very rare case that a GPU block is not found within the 3 consecutive locations the request is not inserted into the cache.

\par We now summarize the insertion policy followed by chaining in DRAMCache. For all CPU fill (insertion) requests, the data is always inserted in the original location (block), and the victim block is evicted removing the chaining, if any, using the reverse chain bit. For a GPU fill request, if the low threshold mark for CPU occupancy is not reached, then the chaining scheme replaces a CPU location, either from the original location or from the chained location, as indicated in row 1 of Table \ref{chaining-replacement}. For a GPU fill request, if the original location belongs to GPU and does not have a chained location, then block is inserted in one of the nearest block [$(B+1)\%N_s$ or $(B+2)\%N_s$ or $(B+3)\%N_s$]. If the original location is chained, then the scheme replaces the chained location, if that belong to CPU or the original location itself, as indicated in row 2 of Table \ref{chaining-replacement}. When the CPU occupancy has hit low threshold, then a GPU fill request replaces the original location if it belongs to GPU (row 4 in Table \ref{chaining-replacement}). If the original location belongs to CPU, and does not have a chained block, then the GPU request is chained to the next nearest location. If the original location is chained, but the chained location belongs to GPU, then the fill request replaces that. Otherwise, the fill request is not inserted in the DRAMCache (see row 3 in Table \ref{chaining-replacement}). Thus the chaining scheme ensures, as far as possible, the CPU requests can find the data in the original location, while the GPU requests attempt to exploit pseudo associativity for increased cache occupancy. In all cases (for both GPU and CPU requests) the access is satisfied with at most 2 tag match, either in original location or in the chained location (identified by the chain bits). 


\par In essence, \cachename uses this set chaining mechanism to force occupancy control in the DRAMCache. Chaining is able to 
(i) ensure a minimum occupancy for the CPU lines in the DRAMCache while effectively allowing the GPU to occupy the rest of the cache by providing pseudo-associativity. 
(ii) remain as a direct mapped cache for majority of the CPU requests
(iii) avoid forcing eviction of hot GPU lines while also avoiding storing of dead lines in the cache.
Lastly, this scheme is dynamic and allows to adaptively set CPU occupancy threshold \textit{$l_{cpu}$} based on the workloads requirements. This occupancy control mechanism does not incur any additional storage and uses the unused space in the DRAMCache rows. Once the GPU finishes kernel execution \cachename returns to a direct mapped cache as the lines inserted into the DRAMCache occupy chained location thereby unlinking chains. 


\begin{table}[]
\centering
\small
\input{chaining-table}
\caption{GPU Fill request insertion policy. Threshold reached : $O_{gpu}\le(1-l_{cpu})$ where $O_{gpu}$ is the current GPU occupancy in DRAMCache row}
\label{chaining-replacement}
\end{table}
