\section{\cachename for IHS} \label{design+mechanism}

\cachename builds on an aggressive direct mapped cache design with tags stored in DRAM as TAD units. \cachename caches lines at 128B granularity. In this section, we first rationalize these design decisions.

\subsection{\cachename Design} \label{design}

\par \textbf{Metadata overhead:} The metadata requirement for DRAMCaches, even for caches of size 64MB, is large and is in the order of few MBs. The large storage requirement along with the associated cost if it has to be stored in SRAM, has driven DRAMCache designs to either use larger block sizes (of size 2KB or 4KB \cite{footprint,unison-cache}) to reduce metadata overhead or co-locate metadata alongside data in the DRAMCache \cite{loh-hill,alloy,atcache}. In the former case, misses waste precious off-chip bandwidth in the absence of spatial locality while the latter design faces tag-serialization. 
\par To understand the spatial locality characteristics of large blocks in a IHS configuration, we experimented with 512 byte block organization for the DRAMCache. However, the tag for the sub-blocks is replicated and stored for every 128 byte block. This simple organization achieves the benefits of a single access for tag-and-data \cite{alloy} for cache lookups, albiet at the cost of some wasted storage for the replicated tags. Figure \ref{fig:design-bigblock} plots the performance achieved by the 512B block cache (in terms of Harmonic Mean of IPC) normalized to that of a 128B block cache. We observe that the large block cache under performs the smaller block cache by 12.2\% and 10.6\% for the CPU and GPU respectively. Despite a 11\% and 8\% improvement in hit rate in the DRAMCache for CPU and GPU respectively the average memory access latency increases by an average of 75\% for the larger block cache. The increased hit rate comes at the cost of wasted bandwidth and increased off-chip DRAM latency as some of the sub blocks fetched are not used. We observe that on an average for 65\% of the 512B blocks that are brought into the cache, upto 2 sub blocks are used. 
\par Tags-in-DRAM designs however have further focused on improving access latencies by removing tag-serialization overhead using overlapped tag lookups \cite{loh-hill} or storing Tag-and-Data units \cite{alloy} . These designs come close to tags-in-SRAM like access latency without concerns of spatial locality characteristics of large block sizes. 
Hence, \cachename organizes data at 128 byte block size and stores data in DRAMCache as a cohesive TAD unit.

\begin{figure}[htbp]
   \includegraphics[scale=1.0]{graphs/design-bigblock}
   \caption{Performance of 512B vs 128B block size}	
   \label{fig:design-bigblock}
\end{figure}

\par \textbf{Associativity:} Providing set associativity is known to improve cache hit rates by reducing conflict misses. In DRAMCaches where tag is stored in DRAM,  associativity comes at a cost. Hit latencies increase due to the tag requiring to be burst out of the stacked DRAM. Hence there is an implicit trade-off between providing better hit-rates and reduced access latency. A higher associativity design is suitable for a GPGPU processor which can trade increased access latency for higher hit rates to make better use the larger bandwidth of the DRAMCache. On the other hand, a CPU would suffer when using such a design but would benefit from a latency-optimized direct-mapped cache \cite{alloy}. The large performance decline for CPUs while the passivity of GPUs to co-running require \cachename to be organized as a direct mapped cache to achieve better hit time for improved CPU performance. Further, this simplifies the design and eschews the need for tag-caches \cite{atcache} and way locators \cite{bimodal} to improve hit times. \cachename's organization is also inline with the commercially adopted stacked MCDRAM on the Knights Landing generation of the Intel XeonPhi processor \cite{xeonphi} where the DRAMCache is organized as a direct mapped cache.

\par \textbf{Miss Penalty:} The access latency provided by a stacked DRAM is only slightly better compared to off-chip DRAMs. Hence, the misses experience upto 1.7 times the DRAM access latency, since a miss in the DRAMCache would then accesses the memory serially. To overcome this, researchers have proposed cache line hit predictors \cite{loh-hill,alloy} which are critical to extract performance from DRAMCaches. These predictors start an early access to memory on the probability that the block will miss in the DRAMCache. Intuitively, we apply the MAP-I prediction \cite{alloy} to CPU requests to start early memory access when an access is predicted to be a miss. GPU requests always proceed serially through the cache after verifying via tag match. This helps (a) reduce the wastage of off-chip bandwidth for mis-predictions for GPU (b) avoid large structures that will be required for making reliable predictions for GPUs which might require correlating warp, thread, and CU IDs.

\par \textbf{Row Buffer Hits (RBH) vs Bank Level Parallelism (BLP):} Stacked DRAMs are organized as vaults (channels), layers (ranks) and banks within each layer as shown in Figure \ref{fig:stackdram}. Each vault has several TSVs which constitute lanes in a channel. Stacked DRAMs provide large bandwidth by organizing DRAMs as several smaller banks within layers. Given this abundant BLP, should DRAMCaches exploit this parallelism over improved RBH? In other words, should the addressing scheme of a DRAMCache be organized as RoCoRaBaCh (Row,Column,Rank,Bank,Channel) -- referred to as the BLP-scheme -- which distributes the cache blocks in banks of different ranks as opposed to RoRaBaCoCh -- referred to as the RBH-scheme -- which stores cache blocks consecutively in the row of a bank. We plot the performance of an IHS processor with DRAMCache for both the addressing schemes in figure \ref{fig:design-rbhblp}. We observe that both the CPU and GPU perform on an average 3\% and 1\% worse respectively using the BLP scheme. Consequently, \cachename is addressed using a RBH friendly addressing scheme (RoRaBaCoCh scheme). 

\begin{figure}[htbp]
   \includegraphics[scale=1.0]{graphs/design-rbhblp}
   \caption{Performance of BLP-scheme vs RBH-scheme}
   \label{fig:design-rbhblp}
\end{figure}

\par Lastly, as shown in Figure \ref{fig:motivation} despite the addition of such a carefully designed DRAMCache, the CPU still suffers significant performance losses while the GPU is relatively unaffected in an IHS architecture, compared to when they run alone. GPUs capability to context switching warps make it more latency tolerant. This suggests that the DRAMCache should be optimized to regain the lost CPU performance without compromising the GPU performance.  In the following subsection we propose 3 schemes for achieving this. 