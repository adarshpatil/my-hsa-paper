\section{Related Work} \label{related-work}
State-of-the-art stacked DRAM device designs have primarily focused on improving the performance of multi-core architectures. %Principally there have been 2 different approaches for using the stacked DRAM devices. 
In \cite{pom,cameo} the stacked DRAMs are organized as part of memory in view of the large capacities provided by these devices. The designs propose hardware management schemes for swapping hot pages into and out of the stacked DRAM devices.
On the other hand, several designs propose to use the stacked DRAM as transparent hardware managed cache. Sectored caches such as \cite{footprint,unison-cache} allocate large blocks and avoid wastage of bandwidth by intelligently fetching useful blocks only. In Section \ref{design} we have broadly alluded to some of the works of DRAMCache designs and organizations \cite{alloy,atcache,bimodal,loh-hill} that organize DRAMCache at smaller system-sized blocks (64B or 128B).
%There are a number of designs around organizing DRAMCache at smaller system-sized blocks (64B or 128B) or an intermediate block size (512B, 1024B or 2048B). 
These designs intelligently manage metadata and tag lookup serialization by using approximate hardware structures. \cachename\ extends and adapts the simplicity and effectiveness of the Alloy Cache organization for IHS architecture after carefully considering the implications of each design decision on performance. 
While these works have focused on improving multi-core CPU performance, our work shows that there exists a large potential for performance improvements by using stacked DRAMCache in a IHS architecture. 
\par Orthogonal to these efforts, there have also been proposals such as \cite{software-dram} to expose the stacked DRAM to the applications and/or system software by providing special allocation calls or using intelligent page management algorithms in system software to place hot data in the high bandwidth memory. Besides these designs incurring the obvious overheads of software modification to improve performance, they also require a good understanding of program behavior and IHS architecture knowledge.
\par Recently, researchers have pointed out the need to utilize all the available bandwidth from both on-chip and off-chip DRAM devices due to comparable access latencies \cite{mostly-clean,mainak-hpca,bear,micro-refresh,mostly-clean-direct} to extract best performance and improve resource utilization. However, \cachename\ uses the ingrained disparity in the requests rates and their implication on performance of each core to optimize bandwidth balance in a heterogeneity aware manner. 
\par Complementary to our work, there have been efforts to improve performance of IHS systems in \cite{gpu-concurrency} by throttling the GPU cores using intelligent warp scheduling and avoiding congestion in the network-on-chip \cite{interconnect}. These techniques are orthogonal to ours which explores managing the interference in memory subsystem of IHS architectures. To manage shared on-chip SRAM caches, Lee et al. \cite{tap} propose heterogeneity aware schemes that are built on top of UCP and RRIP schemes for managing shared  resources. While our chaining mechanism is in line to this to ensure minimum occupancy for CPU requests, it also goes beyond by introducing pseudo-associativity and improving hit-rates (specifically for GPU requests). Mekkat et al. \cite{helm} further propose shared SRAM cache management for IHS workloads that uses runtime metrics, like cache sensitivity of each workload, to allocate cache capacities. Despite larger capacities, DRAMCaches have higher latencies and hence will not be able to adapt quickly to SRAM occupancy management schemes proposed in these works.
\par Zhan et al. \cite{oscar} propose improving performance of IHS architectures by replacing on-chip SRAM caches with slightly larger STT-SRAM caches that are non-volatile but have asymmetric read/write energy and latencies. They focus on NoC related optimizations through NoC reordering/batching schemes and differential CPU/GPU, read/write prioritization. The NoC optimizations are orthogonal and can be supplemented to the ideas proposed in this work. The performance improvement due to introduction of STT-RAM in IHS architectures is equivalent to that observed in our naive DRAMCache.
%\par Lastly, Ausavarungnirun et al., \cite{sms} propose staged memory scheduling for main memory (DRAM) in IHS processors and is similar in spirit to our \prioname, which is applied at the DRAMCache. \prioname uses a relatively simpler approach for prioritizing CPU requests while ensuring that GPU request are not significantly starved. 
\par The authors in \cite{qos-aware} propose a QoS aware memory scheduler to avoid the GPU from missing a frame rendering deadline. However, in our IHS architecture the GPU is used to accelerate general purpose code and hence \prioname\ does not consider such deadlines for the memory controller.
%TACO REVIEW CHANGES HERE 
\par In \cite{umh}, the authors propose to use the GPU side stacked DRAM as a another level of cache for the GPU memory hierarchy before going to the host memory. They use additional structures to maintain memory coherence information both on the CPU and discrete GPU side. However, complementary to our approach, this stacked DRAMCache on the discrete GPU is primarily used for caching data from the GPU cores while \cachename\ is shared by both CPU and the integrated GPU throughout the execution. Several of our design decisions include trade-offs which are heterogeneity-aware and provide improved access latency for CPU cores as well as high bandwidth for GPU cores.
\par Workload suites such as Chai \cite{chai} and Hetero-Mark \cite{hetero-mark} were designed to collaboratively engage both CPU and GPU cores simultaneously and take full advantage of the IHS architecture. The Chai benchmark suite was developed independently and concurrent to this work.
% should we mention HSC coherence paper or sNeha agarwal papers?