\section{\cachename\ for IHS} \label{design+mechanism}
\cachename\ uses HMC-like \cite{hmc} stacked DRAM memory
\footnote{
There are 2 commercial implementations of the stacked DRAM available on the market today. 
Hybrid Memory Cube (HMC) is a joint Intel-Micron standard that is not yet a JEDEC standard. 
HMC has a logic die (ASIC) at the base of the stacked DRAM that houses the control logic for the DRAM stacks (as implemented in the Xeon Phi Knights Landing processor\cite{xeonphi}). 
On the other hand there is High Bandwidth Memory (HBM) proposed by AMD, NVIDIA and Hynix has been ratified by JEDEC. 
HBM tightly couples the host compute die with an interface that is divided into independent channels.
HBM is slated to be used in NVIDIA Pascal GPU and next generation AMD APU/GPU.
Both these technologies promise high bandwidth upto 250 GBps using TSV interconnects. The only difference in the technology would be the signalling interface. While HMC uses a SERDES interface to its stacked DRAMs logic die, HBM uses a traditional DDR signalling interface to the stacked DRAM chip.
\par Our evaluation in this work used a HMC-like model and timing parameters. However, \cachename\ is flexible enough to be adapted using either technology. \cachename\ addressing schemes, mechanisms and associated metadata are stored in the DRAMCache controller. In case of HMC this would be implemented at the logic die while the same would be implemented at the host side stacked DRAM controller in case of HBM.
}. It adapts an aggressive direct mapped cache design with tags stored in DRAM as TAD units \cite{alloy} with a cache line size of 128 byte. In this section, we first rationalize these design decisions.
\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.6]{graphs/motivation-cpu-partition}
	\includegraphics[scale=0.6]{graphs/motivation-gpu-partition}
	\caption{Performance comparison of (a) CPU (b) GPU when with un-partitioned D\$ and partitioned D\$}
	\label{fig:motivation-partition}
\end{figure}
\subsection{\cachename\ Design} \label{design}

\par \textbf{Metadata overhead:} The metadata requirement for DRAMCaches, even for caches of size 64MB, is large and is in the order of few MBs. The large storage requirement along with the associated cost if it has to be stored in SRAM, has driven DRAMCache designs to either use larger block sizes (of size 2KB or 4KB \cite{footprint,unison-cache}) to reduce metadata overhead or co-locate metadata alongside data in the DRAMCache \cite{loh-hill,alloy,atcache}. 
%In the former case, misses waste precious off-chip bandwidth in the absence of spatial locality while the latter design faces tag-serialization. 
\par To understand the spatial locality characteristics of large blocks in a IHS configuration, we experimented with 512 byte block organization for the DRAMCache. 
%TACO REVIEW CHANGES HERE
The 512 byte block size provides a good trade-off between increased hit rates and reduced metadata for multi-core CPU workloads \cite{bimodal}. While we increase the cache line size of the DRAMCache, the higher caches (CPU and GPU L2) still operate at 128 byte cache line size i.e. requests to and responses from the DRAMCache are for 128 byte cache lines. Thus one 512 byte cache block in the DRAMCache has four 128 byte sub-blocks that can be requested by higher level caches. When a requested cache line is not found in the DRAMCache, a request for the corresponding 512 byte aligned cache block is sent to DRAM memory. Bringing in a larger block (512 bye) can improve the cache hit ratio (due to spatial locality), the miss penalty (of the DRAMCache) also increases due to the increased fetch and data transfer. 
%However, the tag for the sub-blocks is stored for every 128 byte block separately. This simple organization achieves the benefits of a single access for tag-and-data \cite{alloy} for cache lookups, albiet at the cost of some wasted storage for the replicated tags. We observe that the 512 byte block under performs the 128 byte block cache by 12.2\% and 10.6\% for the CPU and GPU respectively. Despite a 11\% and 8\% improvement in hit rate in the DRAMCache for CPU and GPU respectively the average memory access latency increases by an average of 75\% for the 512 byte block cache. The increased hit rate comes at the cost of wasted bandwidth and increased off-chip DRAM latency as some of the sub blocks fetched are not used. 
\par We evaluate how much of the data brought by the DRAMCache is actually used by the higher level caches in this setup. We find that on an average for 65\% of the 512B blocks that are brought into the cache, at most 2 sub blocks of 128 byte are requested/used.
This implies wasted capacity in the DRAMCache and wasted off-chip bandwidth. Figure \ref{fig:design-bigblock} shows, on the primary y-axis, the performance of CPU and GPU with a 512 byte block size DRAMCache, in terms of harmonic mean of IPCs normalized to that of 128 byte block size DRAMCache.
Although with the increased cache line size of 512 byte, we observe a 11\% and 8\% improvement in hit rate in the DRAMCache (not shown in Figure) for CPU and GPU respectively, the performance degrades by 12.2\% and 10.6\%, respectively, for the CPU and GPU cores compared to a 128 byte block organization. The poor performance of a large cache block size is attributed to increased average access latency of the DRAMCache (due to larger block size fetch) and the wasted off-chip bandwidth. The secondary y-axis in Figure \ref{fig:design-bigblock} shows the percentage increase in memory access latency for the 512 byte DRAMCache organization over the 128 byte DRAMCache organization. We observe that memory access latency increases by an average of 75\% due to the unused data fetched.
%\footnote{Efficient sub-block management in this context is complicated as the both the memory and cache are slower DRAM devices. We leave the exploration of critical word first and early restart for reducing miss penalty to future work}
\par Tags-in-DRAM designs have further focused on improving access latencies by removing tag-serialization overhead using overlapped tag lookups \cite{loh-hill} or storing TAD units \cite{alloy}. These designs come close to tags-in-SRAM like access latency without concerns of spatial locality characteristics of large block sizes. 
Hence, \cachename\ organizes data at 128 byte block size and stores data in DRAMCache as a cohesive TAD unit.

\begin{figure}[htbp]
   \includegraphics[scale=0.8]{graphs/design-bigblock}
   \caption{Performance of 512B vs 128B block size}	
   \label{fig:design-bigblock}
\end{figure}

\par \textbf{Associativity:} Providing set associativity is known to improve cache hit rates by reducing conflict misses. In DRAMCaches where tag is stored in DRAM,  associativity comes at a cost. Hit latencies increase due to the tag requiring to be burst out of the stacked DRAM. Hence there is an implicit trade-off between providing better hit-rates and reduced access latency. A higher associativity design is suitable for a GPGPU processor which can trade increased access latency for higher hit rates to make better use of the larger bandwidth of the DRAMCache. On the other hand, the CPU would suffer when using such a design due to the increased hit-latency, and  would instead prefer a latency-optimized direct-mapped cache \cite{alloy}.
%The large performance decline for CPUs due to co-running requires \cachename to be organized as a direct mapped cache to achieve better hit time for improved CPU performance. 
Further, such a direct mapped organization simplifies the design and eschews the need for tag-caches \cite{atcache} and way locators \cite{bimodal} to improve hit times. \cachename's organization is also inline with the commercially adopted stacked MCDRAM on the Knights Landing generation of the Intel XeonPhi processor \cite{xeonphi} where the DRAMCache is organized as a direct mapped cache.

\par \textbf{Miss Penalty:} The access latency provided by a stacked DRAM is only slightly better (say 0.7x
\footnote{We model our DRAM timings for stacked DRAM from DRAMSpec\cite{dramspec} HMC model where for example $t_{cl}$ for DDR3 is 13.75ns and that for HMC is 9.9ns (\textasciitilde70\% of DDR3). Detailed timings can be found in Section \ref{methodology}. Prior works assume latencies varying from 0.5x\cite{alloy} to 1.0x\cite{mainak-hpca}. Further, stacked DRAM latencies, for example in Intel MCDRAM\cite{xeonphi}, are very similar to off-chip DRAM. Given the smaller size of stacked DRAM, we feel that our assumption of 0.7x is a reasonable midpoint.  Further, we perform sensitivity study w.r.t. the relative access latencies for off-chip and stacked DRAM in Section \ref{sensitivity:latency}}
) compared to off-chip DRAMs. Thus a miss in the DRAMCache would experience a delay of 1.7x as the DRAM (memory) access takes place after the miss is detected (serially). To overcome this, researchers have proposed cache line hit predictors \cite{loh-hill,alloy} which are critical to extract performance from DRAMCaches. These predictors start an early access to memory if they predict that the block will miss in the DRAMCache. Intuitively, we apply the MAP-I prediction \cite{alloy} to CPU requests to start early memory access when an access is predicted to be a miss. GPU requests always proceed serially through the cache after verifying via tag match. This helps to (a) reduce the wastage of off-chip bandwidth for mis-predictions for GPU (b) avoid large structures that will be required for making reliable predictions for GPUs which might require correlating warp, thread, and CU IDs.

\par \textbf{Row Buffer Hits (RBH) vs Bank Level Parallelism (BLP):} Stacked DRAMs are organized as vaults (channels), layers (ranks) and banks within each layer as shown in Figure \ref{fig:hsa-arch}(b). Each vault has several TSVs which constitute lanes in a channel. Stacked DRAMs provide large bandwidth by organizing DRAMs as several smaller banks within layers. Given this abundant BLP, should DRAMCaches exploit this parallelism over improved RBH? In other words, should the addressing scheme of a DRAMCache be organized as RoCoRaBaCh (Row,Column,Rank,Bank,Channel) - referred to as the BLP-scheme - which distributes the cache blocks in banks of different ranks as opposed to RoRaBaCoCh - referred to as the RBH-scheme - which stores cache blocks consecutively in the row of a bank. We experimented with both the addressing schemes for an IHS processor with a DRAMCache and observe that both the CPU and GPU perform on an average 3\% and 1\% worse respectively when using the BLP scheme. Consequently, \cachename\ is addressed using a RBH friendly addressing scheme (RoRaBaCoCh scheme). 

%\begin{figure}[htbp]
%   \includegraphics[scale=1.0]{graphs/design-rbhblp}
%   \caption{Performance of BLP-scheme vs RBH-scheme}
%   \label{fig:design-rbhblp}
%\end{figure}

%\par Lastly, as shown in Figure \ref{fig:motivation} despite the addition of such a carefully designed DRAMCache, the CPU still suffers significant performance losses while the GPU is relatively unaffected in an IHS architecture, compared to when they run alone. GPUs capability to context switch between warps make it latency tolerant. This suggests that the DRAMCache should be optimized to regain the lost CPU performance without compromising the GPU performance. In the following subsection we propose three schemes for achieving this.