\section{\cachename\ for IHS} \label{design+mechanism}
\cachename\ adapts an aggressive direct mapped cache design with tags stored in DRAM as TAD units \cite{alloy} with a cache line size of 128 byte. In this section, we first rationalize these design decisions.

\subsection{\cachename\ Design} \label{design}

\par \textbf{Metadata overhead:} The metadata requirement for DRAMCaches, even for caches of size 64MB, is large and is in the order of few MBs. The large storage requirement along with the associated cost if it has to be stored in SRAM, has driven DRAMCache designs to either use larger block sizes (of size 2KB or 4KB \cite{footprint,unison-cache}) to reduce metadata overhead or co-locate metadata alongside data in the DRAMCache \cite{loh-hill,alloy,atcache}. 
%In the former case, misses waste precious off-chip bandwidth in the absence of spatial locality while the latter design faces tag-serialization. 
\par To understand the spatial locality characteristics of large blocks in a IHS configuration, we experimented with 512 byte block organization for the DRAMCache. 
%However, the tag for the sub-blocks is stored for every 128 byte block separately. This simple organization achieves the benefits of a single access for tag-and-data \cite{alloy} for cache lookups, albiet at the cost of some wasted storage for the replicated tags. We observe that the 512 byte block under performs the 128 byte block cache by 12.2\% and 10.6\% for the CPU and GPU respectively. Despite a 11\% and 8\% improvement in hit rate in the DRAMCache for CPU and GPU respectively the average memory access latency increases by an average of 75\% for the 512 byte block cache. The increased hit rate comes at the cost of wasted bandwidth and increased off-chip DRAM latency as some of the sub blocks fetched are not used. 
On an average for 65\% of the 512B blocks that are brought into the cache, at most 2 sub blocks are used.
This implies wasted capacity in the DRAMCache and wasted off-chip bandwidth.  Although with the increased cache line size of 512 byte, we observe a 11\% and 8\% improvement in hit rate in the DRAMCache for CPU and GPU respectively, the performance (normalized H-Mean of IPC) degrades by 12.2\% and 10.6\%, respectively, for the CPU and GPU cores. The poor performance of  a large cache block size is attributed to increased average access latency of the DRAMCache (due to larger block size fetch) and the wasted off-chip bandwidth.
%\footnote{Efficient sub-block management in this context is complicated as the both the memory and cache are slower DRAM devices. We leave the exploration of critical word first and early restart for reducing miss penalty to future work}
\par Tags-in-DRAM designs have further focused on improving access latencies by removing tag-serialization overhead using overlapped tag lookups \cite{loh-hill} or storing TAD units \cite{alloy}. These designs come close to tags-in-SRAM like access latency without concerns of spatial locality characteristics of large block sizes. 
Hence, \cachename\ organizes data at 128 byte block size and stores data in DRAMCache as a cohesive TAD unit.

%\begin{figure}[htbp]
%   \includegraphics[scale=1.0]{graphs/design-bigblock}
%   \caption{Performance of 512B vs 128B block size}	
%   \label{fig:design-bigblock}
%\end{figure}

\par \textbf{Associativity:} Providing set associativity is known to improve cache hit rates by reducing conflict misses. In DRAMCaches where tag is stored in DRAM,  associativity comes at a cost. Hit latencies increase due to the tag requiring to be burst out of the stacked DRAM. Hence there is an implicit trade-off between providing better hit-rates and reduced access latency. A higher associativity design is suitable for a GPGPU processor which can trade increased access latency for higher hit rates to make better use of the larger bandwidth of the DRAMCache. On the other hand, the CPU would suffer when using such a design due to the increased hit-latency, and  would instead prefer a latency-optimized direct-mapped cache \cite{alloy}.
%The large performance decline for CPUs due to co-running requires \cachename to be organized as a direct mapped cache to achieve better hit time for improved CPU performance. 
Further, such a direct mapped organization simplifies the design and eschews the need for tag-caches \cite{atcache} and way locators \cite{bimodal} to improve hit times. \cachename's organization is also inline with the commercially adopted stacked MCDRAM on the Knights Landing generation of the Intel XeonPhi processor \cite{xeonphi} where the DRAMCache is organized as a direct mapped cache.

\par \textbf{Miss Penalty:} The access latency provided by a stacked DRAM is only slightly better (say 0.7x) compared to off-chip DRAMs. Thus a miss in the DRAMCache would experience a delay of 1.7x as the DRAM (memory) access takes place after the miss is detected (serially). To overcome this, researchers have proposed cache line hit predictors \cite{loh-hill,alloy} which are critical to extract performance from DRAMCaches. These predictors start an early access to memory if they predict that the block will miss in the DRAMCache. Intuitively, we apply the MAP-I prediction \cite{alloy} to CPU requests to start early memory access when an access is predicted to be a miss. GPU requests always proceed serially through the cache after verifying via tag match. This helps to (a) reduce the wastage of off-chip bandwidth for mis-predictions for GPU (b) avoid large structures that will be required for making reliable predictions for GPUs which might require correlating warp, thread, and CU IDs.

\par \textbf{Row Buffer Hits (RBH) vs Bank Level Parallelism (BLP):} Stacked DRAMs are organized as vaults (channels), layers (ranks) and banks within each layer as shown in Figure \ref{fig:stackdram}. Each vault has several TSVs which constitute lanes in a channel. Stacked DRAMs provide large bandwidth by organizing DRAMs as several smaller banks within layers. Given this abundant BLP, should DRAMCaches exploit this parallelism over improved RBH? In other words, should the addressing scheme of a DRAMCache be organized as RoCoRaBaCh (Row,Column,Rank,Bank,Channel) - referred to as the BLP-scheme - which distributes the cache blocks in banks of different ranks as opposed to RoRaBaCoCh - referred to as the RBH-scheme - which stores cache blocks consecutively in the row of a bank. We experimented with both the addressing schemes for an IHS processor with a DRAMCache and observe that both the CPU and GPU perform on an average 3\% and 1\% worse respectively when using the BLP scheme. Consequently, \cachename\ is addressed using a RBH friendly addressing scheme (RoRaBaCoCh scheme). 

%\begin{figure}[htbp]
%   \includegraphics[scale=1.0]{graphs/design-rbhblp}
%   \caption{Performance of BLP-scheme vs RBH-scheme}
%   \label{fig:design-rbhblp}
%\end{figure}

%\par Lastly, as shown in Figure \ref{fig:motivation} despite the addition of such a carefully designed DRAMCache, the CPU still suffers significant performance losses while the GPU is relatively unaffected in an IHS architecture, compared to when they run alone. GPUs capability to context switch between warps make it latency tolerant. This suggests that the DRAMCache should be optimized to regain the lost CPU performance without compromising the GPU performance. In the following subsection we propose three schemes for achieving this.