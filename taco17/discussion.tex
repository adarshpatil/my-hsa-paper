\section{Discussion}
\subsection{Reject-Retry Mechanism}
Interactions between the memory components is modeled as a master-slave port architecture. This is common practice is most architectures that involve requests and responses. A master port sends requests while the slave port receives requests and performs appropriate action on the requests. For example, when the directory controller's memory master port sends a request to the slave port of DRAMCache Controller it enqueues the request in the appropriate queue (read or write queue). Similarly, a slave port sends out responses and the master port receives these responses and performs appropriate action. For example, when the DRAM controller responds to the a read request, the DRAMCache controller clears the corresponding MSHR and sends a response to the requester.
\par However, a cache can be in a blocked state when MSHRs or targets within a MSHR or WriteBuffers are unavailable. At such a time, when a request is received at the DRAMCache slave port but is not able to act on it due to the cache being blocked or read/write queue being full or due to mechanism restrictions as in \prioname, the request is rejected and the reason for the blocking is remembered (i.e., the resource causing the block). When the corresponding resource becomes available, a retry message is sent to the master port from the slave port. Once the master port receives this message, the request is resent. This is called the reject-retry mechanism. This mechanism is independent of the network-on-chip that handles routing, channel partitioning etc.
\subsection{HBM vs HMC}
HMC - xeon phi 
ASIC at the base of the HMC to manage the DRAM package.
ASIC) at the base of the HMC houses the circuitry to manage the DRAMs on top of it -- freeing up room on each DRAM. Also, the ASIC manages the communications of data on and off the chip. The interconnections between the ASIC and the DRAM die above it use up to 2,000 through-silicon vias (TSVs). The ASIC takes over many of the logic functions from each DRAM, freeing up room for TSVs without enlarging the die.

HBM - Hynix, AMD, and Nvidia. - standardized by JEDEC
NVIDIA and AMD will use in next generation of GPU
AMD has talked about building APU with HBM

HBM has an architecture that can include a processor (often a GPU) in the same package, providing a tightly-coupled high-speed processing unit. HMC devices, on the other hand, are intended to be connected up to a processor (or "host") with the ability to "chain" multiple memories for higher capacity.
\subsection{Memory-Side Cache}
thus does not 
refer MICRO paper of Intel and Mainak
introduce another level of coherence issues. The LLC is logically just in front of the memory and serves 
to reduce the average latency of memory accesses and increase the memoryâ€™s effective bandwidth.

memory-side caches at that level do not need to be snooped since they cannot cache each others partition of memory.

while the dram cache is notionally just in front of memory, it is still coherent here. I will try to clarify that it does not add states to the coherence protocol and there is no need to snoop it separately and a memory state is enough to serve data from it. I will also try to find a better reference.
\subsection{Coherence/Memory Consistency}
\subsection{Network-on-Chip}
The NoC is modeled as a hierarchical cross-bar topology. One cross bar connect the CPU and caches to each other and another cross bar connects the GPU to the caches. A third cross bar connects the Last Level Caches of the CPU and GPU to the DRAMCache memory controller.
\subsection{Benchmarks}
Workload suites such as Chai \cite{chai} and Hetero-Mark \cite{hetero-mark} were designed to collaboratively engage both CPU and GPU cores simultaneously and take full advantage of the IHS architecture. The Chai benchmark suite was developed independent and concurrent to this work requiring changes to our baseline simulator.
The Hetero-Mark benchmark suite has not been developed for running in our gem5-gpu simulator which impeded usage of these workloads. To simulate the simultaneous activity on both CPU and GPU cores we use co-running multi-programmed workload with Rodinia benchmark and memory traffic interleaving at the stacked DRAM and off-chip DRAM.
However broadly speaking we expect the results to be similar to the chai benchmarks.
