\section{Discussion}
\subsection{Reject-Retry Mechanism}
Interactions between the memory components is modeled as a master-slave port architecture. This is common practice is most architectures that involve requests and responses. A master port sends requests while the slave port receives requests and performs appropriate action on the requests. For example, when the directory controller's memory master port sends a request to the slave port of DRAMCache Controller it enqueues the request in the appropriate queue (read or write queue). Similarly, a slave port sends out responses and the master port receives these responses and performs appropriate action. For example, when the DRAM controller responds to the a read request, the DRAMCache controller clears the corresponding MSHR and sends a response to the requester.
\par However, a cache can be in a blocked state when MSHRs or targets within a MSHR or WriteBuffers are unavailable. At such a time, when a request is received at the DRAMCache slave port but is not able to act on it due to the cache being blocked or read/write queue being full or due to mechanism restrictions as in \prioname, the request is rejected and the reason for the blocking is remembered (i.e., the resource causing the block). When the corresponding resource becomes available, a retry message is sent to the master port from the slave port. Once the master port receives this message, the request is resent. This is called the reject-retry mechanism. This mechanism is independent of the network-on-chip that handles routing, channel partitioning etc.

\subsection{HBM vs HMC}
There are 2 commercial implementations of the stacked DRAM available on the market today. 
Hybrid Memory Cube (HMC) is a joint Intel-Micron standard that is not yet a JEDEC standard. 
HMC has a logic die (ASIC) at the base of the stacked DRAM that houses the control logic for the DRAM stacks (as implemented in the Xeon Phi Knights Landing processor\cite{xeonphi}). 
On the other hand there is High Bandwidth Memory (HBM) proposed by AMD, NVIDIA and Hynix has been ratified by JEDEC. 
HBM tightly couples the host compute die with an interface that is divided into independent channels.
HBM is slated to be used in NVIDIA Pascal GPU and next generation AMD APU/GPU.
Both these technologies promise high bandwidth upto 250 GBps using TSV interconnects. The only difference in the technology would be the signaling interface. While HMC uses a SERDES interface to its stacked DRAMs logic die, HBM uses a traditional DDR signaling interface to the stacked DRAM chip.
\par Our evaluation in this work used a HMC-like model and timing parameters. However, \cachename\ is flexible enough to be implemented using either technology. \cachename\ addressing schemes, mechanisms and associated metadata are stored in the DRAM controller. In case of HMC this would be implemented at the logic die while the same would be implemented at the host side stacked DRAM controller in case of HBM.

\subsection{Coherence and Memory Consistency} \label{discussion:coherence}
The DRAMCache evaluated here is a memory-side cache \cite{primer-coherence-consistency, mainak-hpca, skylake}.  These caches are outside the coherence domain. These caches do not require to add additional states to the coherence protocol and do not need to be snooped separately. They are logically just in front of the memory and serve to reduce the average latency of memory accesses and increase the memory's effective bandwidth.  
\par Each processor consistency is dictated by the host processor architecture i.e. strong memory consistency for the CPU and weak/release consistency for GPU. Cache line evictions and explicit fence operations flush data from L2 SRAM cache to memory and could be cached in the DRAMCache. Subsequent requests to memory for that cache line first look up the DRAMCache (or bypass in case of \bypassname) before sending the request to the off-chip DRAM. Further discussion about cache coherence and memory consistency models for IHS architecture is beyond the scope of this work.

\subsection{Stacked DRAMCaches and on-chip SRAM / STT-RAM caches}
SRAM has traditionally been used for smaller caches closer to the processing elements because of the need for fast access times.
However, modern last level SRAM caches typically have capacities between 2 to 8 MB \cite{skylake} and this is often not large enough to contain the working set of IHS workloads.
Moreover, GPUs have large number of concurrent threads and their access patterns thrash the LLC, removing useful information from the cache due to its small size. 
Non volatile STT-RAM caches have been proposed in recent literature as replacement for on-chip SRAM caches \cite{oscar}. STT-RAM caches provide about 4X the capacity of SRAM \cite{oscar} (in the order of 32MB) for the same area due to its higher cell density.
Stacked DRAMCaches offer much high capacities than its SRAM and STT-RAM counterparts by one to two orders of magnitude \cite{3d-stacked}.
\par Secondly, stacked DRAMCaches are attractive because they provide much larger bandwidth than that of an on-chip SRAM and STT RAM caches.
This is especially compelling as IHS architecture pack multiple CPU and GPU cores have higher memory bandwidth demands and these inter-die vias and interconnects in the 3D chips can supply these data hungry processors.
Stacked DRAMs are not constrained by the bandwidth wall problem as the TSV interface between the DRAM stacks and the logic chip scales with the surface area of the chip rather than with the number of pins at its perimeter. This allows direct, parallel access to the DRAM modules than accessing the memory external to the chip.
\par Stacked DRAM can also be used in conjunction with the on-chip SRAM/STT-RAM caches to augment the memory with higher capacities and bandwidth. We expect the key observations of the heterogeneous nature of the requests and the inferences thereof to be similar even with the shared on-chip caches in IHS architectures.


\subsection{GPU request starvation in \prioname}
\prioname\ picks requests to be scheduled from the input queue. The exact selection of input queue (Read, Write or Fill Queue) is done external to this algorithm based on certain heuristics and constraints which is beyond the scope of this scheduler algorithm. Broadly, the algorithm picks one of the 3 types of requests; (a) seamless row buffer hit, (b) hidden bank prep or (c) prepped row, in that strict order of priority. 
\par A seamless row buffer hit request refers to a request that can issue a column access to an already activated row in the bank, without any further delay. A hidden bank prep request is a request that can overlap the current operation in other banks (in the same rank) and issue a request to activate or precharge a row in the requested bank. Among the hidden bank prep requests an FCFS policy is followed. A prepped row request refers to a request that needs to wait for the current column access to complete to the currently active row in a bank. Thus choosing a prepped row leads to a bubble in the pipeline of the scheduler where the request has to wait for the row to become available for a column access command.

\par Additionally, the \prioname\ algorithm picks a request in the priority order of
\begin{quote}
	CPU seamless row buffer hit $>$ CPU hidden bank prep $>$ CPU prepped row $>$ GPU seamless row buffer hit $>$ GPU hidden bank prep $>$ GPU prepped row buffer hit
\end{quote}
We experimented with several combinations of this priority order and find that prioritizing CPU requests at all levels provides best performance for CPU requests while the reduction in GPU performance due to de-prioritization between the schemes was not significant.
\par Thus, \prioname\ is able to reduce access latencies by reducing waiting delay and queue latencies for CPU requests at DRAMCache. While using \prioname\ within reasonable thresholds (X cycles), the GPU shows no forward progress error (deadlocks)  i.e. all memory requests return within the specified threshold, for any of the benchmarks. This suggests that the applied heuristic in \prioname\ allows such request service reordering without causing deadlocks.

\subsection{Network-on-Chip}
The NoC is modeled as a hierarchical cross-bar topology. One cross bar connects the CPU and caches to each other and another cross bar connects the GPU to the caches. A third cross bar connects the Last Level Caches of the CPU and GPU to the DRAMCache memory controller. The DRAMCache controller does the request scheduling and dispatches commands to the stacked DRAM devices. The DRAMCache controller and off-chip DRAM controller are connected by a point-to-point link. The DRAMCache controller forwards request to the off-chip DRAM controller if necessary.

\subsection{Benchmarks}
Workload suites such as Chai \cite{chai} and Hetero-Mark \cite{hetero-mark} were designed to collaboratively engage both CPU and GPU cores simultaneously and take full advantage of the IHS architecture. The Chai benchmark suite was developed independently and concurrent to this work. Chai benchmarks require changes to our baseline simulator and hence we do not include this in our evaluation.
The Hetero-Mark benchmark suite has not been developed for running in gem5-gpu simulator which impeded the usage of these workloads. 
\par Not withstanding, our simulation of the simultaneous activity on both CPU and GPU cores using co-running multi-programmed workload along with Rodinia-nocopy benchmarks allows us to demonstrate the interleaving of memory traffic at the stacked DRAM and off-chip DRAM. Such a setup albeit does not share significant data through coherence or memory as in Chai, yet it allows us to stress the memory system, showing the heterogeneous nature of the requests itself i.e. latency sensitive vs throughput oriented and the idiosyncratic aware handling required from the memory system.
Broadly speaking we expect the inference and arguments made in this work to be similar with the Chai or Hetero-Mark benchmarks.
