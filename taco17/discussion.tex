\section{Discussion}
\subsection{Reject-Retry Mechanism}
Interactions between the memory components is modeled as a master-slave port architecture. This is common practice is most architectures that involve requests and responses. A master port sends requests while the slave port receives requests and performs appropriate action on the requests. For example, when the directory controller's memory master port sends a request to the slave port of DRAMCache Controller it enqueues the request in the appropriate queue (read or write queue). Similarly, a slave port sends out responses and the master port receives these responses and performs appropriate action. For example, when the DRAM controller responds to the a read request, the DRAMCache controller clears the corresponding MSHR and sends a response to the requester.
\par However, a cache can be in a blocked state when MSHRs or targets within a MSHR or WriteBuffers are unavailable. At such a time, when a request is received at the DRAMCache slave port but is not able to act on it due to the cache being blocked or read/write queue being full or due to mechanism restrictions as in \prioname, the request is rejected and the reason for the blocking is remembered (i.e., the resource causing the block). When the corresponding resource becomes available, a retry message is sent to the master port from the slave port. Once the master port receives this message, the request is resent. This is called the reject-retry mechanism. This mechanism is independent of the network-on-chip that handles routing, channel partitioning etc.

\subsection{HBM vs HMC}
There are 2 commercial implementations of the stacked DRAM available on the market today. 
Hybrid Memory Cube (HMC) is a joint Intel-Micron standard that is not yet a JEDEC standard. 
HMC has a logic die (ASIC) at the base of the stacked DRAM that houses the control logic for the DRAM stacks and is implemented in the Xeon Phi Knights Landing processor. 
On the other hand there is High Bandwidth Memory (HBM) proposed by AMD, NVIDIA and Hynix has been ratified by JEDEC. 
HBM tightly couples the host compute die with an interface that is divided into independent channels.
HBM is slated to be used in NVIDIA Pascal GPU and next generation AMD APU/GPU.
Both these technologies promise high bandwidth upto 250 GBps using TSV interconnects. The only difference in the technology would be the signaling interface. While HMC uses a SERDES interface to its stacked DRAMs logic die, HBM uses a traditional DDR signaling interface to the stacked DRAM chip.
\par Our evaluation in this work used a HMC-like model and timing parameters. However, \cachename is flexible enough to be implemented using either technology. \cachename mechanisms and the associated metadata are stored in the DRAM controller. In case of HMC this would be implemented at the logic die while the same would be implemented at the host side stacked DRAM controller in case of HBM.

\subsection{Coherence and Memory Consistency} \label{discussion:coherence}
The DRAMCache evaluated here is a memory-side cache \cite{primer-coherence-consistency, mainak-hpca, skylake}.  These caches are outside the coherence domain. These caches do not require to add additional states to the coherence protocol and do not need to be snooped separately. They are logically just in front of the memory and serve to reduce the average latency of memory accesses and increase the memory's effective bandwidth. 
\par 

\subsection{\prioname scheduler GPU starvation}

\subsection{Network-on-Chip}
The NoC is modeled as a hierarchical cross-bar topology. One cross bar connect the CPU and caches to each other and another cross bar connects the GPU to the caches. A third cross bar connects the Last Level Caches of the CPU and GPU to the DRAMCache memory controller.
\subsection{Benchmarks}
Workload suites such as Chai \cite{chai} and Hetero-Mark \cite{hetero-mark} were designed to collaboratively engage both CPU and GPU cores simultaneously and take full advantage of the IHS architecture. The Chai benchmark suite was developed independently and concurrent to this work. Chai benchmarks require changes to our baseline simulator and hence we do not include this in our evaluation.
The Hetero-Mark benchmark suite has not been developed for running in gem5-gpu simulator which impeded the usage of these workloads. 
\par Not withstanding, our simulation of the simultaneous activity on both CPU and GPU cores using co-running multi-programmed workload along with Rodinia-nocopy benchmarks allows us to demonstrate the interleaving of memory traffic at the stacked DRAM and off-chip DRAM. Such a setup albeit does not share significant data through coherence or memory as in Chai, yet it allows us to stress the memory system, showing the heterogeneous nature of the requests itself i.e. latency sensitive vs throughput oriented and the idiosyncratic aware handling required from the memory system.
Broadly speaking we expect the inference and arguments made in this work to be similar with the Chai or Hetero-Mark benchmarks.
